# %% [markdown]
# # DB ì—…ë¡œë“œ(.csv íŒŒì¼ ë§Œë“¤ê¸°)

# %%
import os

# 1. ê²½ë¡œ ì •ì˜ (r ë¶™ì´ëŠ” ê±° ìŠì§€ ë§ˆì„¸ìš”!)
data_path = r"C:\Users\Julia\Downloads\sales_data"

# 2. ì‹¤ì œë¡œ ê·¸ í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
if os.path.exists(data_path):
    print(f"âœ… í´ë”ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤: {data_path}")
else:
    print(f"âŒ í´ë”ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ê²½ë¡œê°€ ì •í™•í•œì§€ í™•ì¸í•´ ì£¼ì„¸ìš”: {data_path}")

# 3. í´ë” ì•ˆì— ë­ê°€ ë“¤ì–´ìˆëŠ”ì§€ 'ë¬´ì¡°ê±´' ë‹¤ ì¶œë ¥í•´ë³´ê¸°
try:
    all_files = os.listdir(data_path)
    print(f"ğŸ“¦ í´ë” ë‚´ ì „ì²´ íŒŒì¼ ëª©ë¡ ({len(all_files)}ê°œ):")
    for f in all_files:
        print(f" - {f}")
except Exception as e:
    print(f"âŒ í´ë” ì ‘ê·¼ ì—ëŸ¬: {e}")

# %%
import os
import glob
import pandas as pd

# 1. íŒŒì¼ì´ ë“¤ì–´ìˆëŠ” ë¡œì»¬ ê²½ë¡œ (rì„ ê¼­ ë¶™ì´ì„¸ìš”!)
data_path = r"C:\Users\Julia\Downloads\sales_data" 

# 2. í•„í„°ë§ ì¡°ê±´ (ì„ íƒí•˜ì‹  ì •ì˜ˆ ë©¤ë²„ë“¤)
target_sectors = [
    'ì„¬ìœ ì œí’ˆ', 'ì™„êµ¬', 'ìš´ë™/ê²½ê¸°ìš©í’ˆ', 'í™”ì¥í’ˆ', 'ë¬¸êµ¬', 'ì„œì ', 
    'ì‹œê³„ë°ê·€ê¸ˆì†', 'ì•ˆê²½', 'ì¼ë°˜ì˜ë¥˜', 'í¸ì˜ì ', 'ë…¸ë˜ë°©', 'ë¯¸ìš©ì‹¤', 
    'ë‹¹êµ¬ì¥', 'ì»¤í”¼-ìŒë£Œ', 'í˜¸í”„-ê°„ì´ì£¼ì ', 'ë¶„ì‹ì „ë¬¸ì ', 'ì¹˜í‚¨ì „ë¬¸ì ', 
    'íŒ¨ìŠ¤íŠ¸í‘¸ë“œì ', 'ì œê³¼ì ', 'ì–‘ì‹ìŒì‹ì ', 'ì¤‘ì‹ìŒì‹ì ', 'í•œì‹ìŒì‹ì '
]

cols_to_keep = [
    'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ_ëª…', 'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…', 
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡', 'ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡', 
    'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡'
]

# 3. íŒŒì¼ ì°¾ê¸°
files = sorted(glob.glob(os.path.join(data_path, "*.csv")))
print(f"ğŸ“¦ ì´ {len(files)}ê°œì˜ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

combined_list = []

for f in files:
    filename = os.path.basename(f)
    print(f"ğŸ” {filename} ì²˜ë¦¬ ì¤‘...", end=" ")
    
    try:
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ê¸° (ì†ë„ í–¥ìƒ)
        df = pd.read_csv(f, encoding='cp949', usecols=cols_to_keep)
        
        # [í•„í„° 1] ê¸°ê°„: 2020ë…„ 4ë¶„ê¸°ë¶€í„° (20201, 20202, 20203 ì œì™¸)
        df = df[~df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].isin([20201, 20202, 20203])]
        
        # [í•„í„° 2] ì—…ì¢…: ì„ íƒí•˜ì‹  ì—…ì¢…ë§Œ
        df = df[df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'].isin(target_sectors)]
        
        combined_list.append(df)
        print(f"-> {len(df)}ê±´ ì¶”ì¶œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"-> âŒ ì—ëŸ¬ ë°œìƒ: {e}")

# 4. í•˜ë‚˜ë¡œ í•©ì¹˜ê³  ì €ì¥
if combined_list:
    final_df = pd.concat(combined_list, ignore_index=True)
    
    # ì—‘ì…€ì—ì„œë„ ì˜ ì—´ë¦¬ë„ë¡ 'utf-8-sig'ë¡œ ì €ì¥
    output_name = "seoul_sales_1030_refined.csv"
    final_df.to_csv(output_name, index=False, encoding='utf-8-sig')
    
    print("\n" + "="*30)
    print(f"ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ëë‚¬ìŠµë‹ˆë‹¤!")
    print(f"ğŸ’¾ ìµœì¢… íŒŒì¼ëª…: {output_name}")
    print(f"ğŸ“Š ì´ ë°ì´í„° í–‰ ìˆ˜: {len(final_df)}ê°œ")
    print("="*30)
else:
    print("\nâŒ ì¶”ì¶œëœ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œì™€ íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")


# %%
import pandas as pd

# 1. ì•„ê¹Œ í•„í„°ë§í•´ì„œ í•©ì³ë‘” íŒŒì¼ ì½ê¸°
input_file = "seoul_sales_1030_refined.csv" 
df = pd.read_csv(input_file)

# 2. ê³µì‹ ì˜ë¬¸ ëª…ì¹­ ë§¤í•‘ ì‚¬ì „ (ë³´ë‚´ì£¼ì‹  ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€)
official_mapping = {
    'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ': 'STDR_YYQU_CD',
    'í–‰ì •ë™_ì½”ë“œ': 'ADSTRD_CD',
    'í–‰ì •ë™_ì½”ë“œ_ëª…': 'ADSTRD_CD_NM',
    'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…': 'SVC_INDUTY_CD_NM',
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'THSMON_SELNG_AMT',
    'ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜': 'THSMON_SELNG_CO',
    'ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡': 'MDWK_SELNG_AMT',
    'ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡': 'WKEND_SELNG_AMT',
    'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê¸ˆì•¡': 'AGRDE_10_SELNG_AMT',
    'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡': 'AGRDE_20_SELNG_AMT',
    'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡': 'AGRDE_30_SELNG_AMT'
}

# 3. ì»¬ëŸ¼ëª… ì¼ê´„ ë³€ê²½
df_official = df.rename(columns=official_mapping)

# 4. Supabase ì—…ë¡œë“œìš© ìµœì¢… íŒŒì¼ ì €ì¥
output_file = "seoul_sales_final_official.csv"
df_official.to_csv(output_file, index=False, encoding='utf-8')

print(f"âœ… ë³€í™˜ ì™„ë£Œ! íŒŒì¼ëª…: {output_file}")
print("-" * 30)
print("ğŸš€ ë°”ë€ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸:")
for col in df_official.columns:
    print(f"- {col}")

# %%
import pandas as pd

# 1. ì•„ê¹Œ ê³µì‹ ëª…ì¹­ìœ¼ë¡œ ë°”ê¾¼ íŒŒì¼ ì½ê¸°
df = pd.read_csv("seoul_sales_final_official.csv")

# 2. 'id'ë¼ëŠ” ì»¬ëŸ¼ì„ ë§¨ ì•ì— ë§Œë“¤ê³  1ë¶€í„° ë²ˆí˜¸ ë§¤ê¸°ê¸°
# df.index + 1 ì€ 0, 1, 2... ëŒ€ì‹  1, 2, 3...ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
df.insert(0, 'id', range(1, len(df) + 1))

# 3. ìµœì¢… ì €ì¥
df.to_csv("seoul_sales_ready_to_upload.csv", index=False, encoding='utf-8')

print(f"âœ… PK ì¶”ê°€ ì™„ë£Œ! ì´ {len(df)}ê°œì˜ í–‰ì— idê°€ ë¶€ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(df[['id', 'STDR_YYQU_CD', 'ADSTRD_CD_NM']].head()) # í™•ì¸ìš© ì¶œë ¥

# %%
import pandas as pd
import numpy as np

# 1. í†µí•©ëœ ìµœì¢… íŒŒì¼ ì½ê¸°
input_file = "seoul_sales_ready_to_upload.csv"
df = pd.read_csv(input_file)

# 2. íŒŒì¼ì„ ëª‡ ê°œë¡œ ë‚˜ëˆŒì§€ ì„¤ì •
num_files = 4
# ì „ì²´ í–‰ ìˆ˜ë¥¼ 4ë¡œ ë‚˜ëˆ„ì–´ ìª¼ê°¤ ì§€ì  ê³„ì‚°
chunks = np.array_split(df, num_files)

print(f"ğŸ“Š ì „ì²´ ë°ì´í„° í–‰ ìˆ˜: {len(df)}ê°œ")

# 3. ìª¼ê°œì§„ ë°ì´í„°ë¥¼ ê°ê° ì €ì¥
for i, chunk in enumerate(chunks):
    output_name = f"upload_part_{i+1}.csv"
    chunk.to_csv(output_name, index=False, encoding='utf-8')
    
    # í™•ì¸ì„ ìœ„í•´ ê° íŒŒì¼ì˜ id ë²”ìœ„ë¥¼ ì¶œë ¥
    start_id = chunk['id'].iloc[0]
    end_id = chunk['id'].iloc[-1]
    print(f"âœ… {output_name} ì €ì¥ ì™„ë£Œ! (id: {start_id} ~ {end_id} / í–‰ ìˆ˜: {len(chunk)})")

print("\nğŸš€ ì´ì œ ìœ„ 4ê°œ íŒŒì¼ì„ ìˆœì„œëŒ€ë¡œ Supabaseì— ì˜¬ë¦¬ì‹œë©´ ë©ë‹ˆë‹¤!")

# %% [markdown]
# # ì¶”ì •ë§¤ì¶œ ì—°ê°„ ì¦ê°ìœ¨ SQL ì¿¼ë¦¬ë¬¸

# %%
-- 1. ê¸°ì¡´ì— ê°™ì€ ì´ë¦„ì˜ í…Œì´ë¸”ì´ ìˆë‹¤ë©´ ì‚­ì œ
-- DROP TABLE IF EXISTS sales_growth;

-- 2. í–‰ì •ë™ ì½”ë“œë¥¼ í¬í•¨í•˜ì—¬ ìƒˆ í…Œì´ë¸” ìƒì„±
CREATE TABLE sales_growth AS
WITH annual_sales AS (
  SELECT
    "ADSTRD_CD", -- ì½”ë“œ ìœ ì§€
    MAX("ADSTRD_CD_NM") AS "ADSTRD_CD_NM", -- ì´ë¦„ì€ ëŒ€í‘œê°’ìœ¼ë¡œ
    CASE
      WHEN "STDR_YYQU_CD" BETWEEN 20204 AND 20213 THEN '2020Q4_2021Q3'
      WHEN "STDR_YYQU_CD" BETWEEN 20214 AND 20223 THEN '2021Q4_2022Q3'
      WHEN "STDR_YYQU_CD" BETWEEN 20224 AND 20233 THEN '2022Q4_2023Q3'
      WHEN "STDR_YYQU_CD" BETWEEN 20234 AND 20243 THEN '2023Q4_2024Q3'
      WHEN "STDR_YYQU_CD" BETWEEN 20244 AND 20253 THEN '2024Q4_2025Q3'
    END AS sales_year,
    SUM("THSMON_SELNG_AMT") AS total_sales
  FROM sales
  GROUP BY "ADSTRD_CD", sales_year
),
pivot_base AS (
  SELECT
    "ADSTRD_CD", -- í”¼ë²— ê¸°ì¤€ì— ì½”ë“œ ì¶”ê°€
    MAX("ADSTRD_CD_NM") AS "í–‰ì •ë™ëª…",
    SUM(total_sales) FILTER (WHERE sales_year = '2020Q4_2021Q3') / 100000000.0 AS s1,
    SUM(total_sales) FILTER (WHERE sales_year = '2021Q4_2022Q3') / 100000000.0 AS s2,
    SUM(total_sales) FILTER (WHERE sales_year = '2022Q4_2023Q3') / 100000000.0 AS s3,
    SUM(total_sales) FILTER (WHERE sales_year = '2023Q4_2024Q3') / 100000000.0 AS s4,
    SUM(total_sales) FILTER (WHERE sales_year = '2024Q4_2025Q3') / 100000000.0 AS s5
  FROM annual_sales
  WHERE sales_year IS NOT NULL
  GROUP BY 1
)
SELECT
  "ADSTRD_CD", -- ì´ì œ í…Œì´ë¸”ì— ì½”ë“œê°€ ë‚¨ìŠµë‹ˆë‹¤.
  "í–‰ì •ë™ëª…",
  ROUND(s1, 1) AS "sales_20Q4_21Q3_100M",
  ROUND(s2, 1) AS "sales_21Q4_22Q3_100M",
  ROUND(((s2 - s1) / NULLIF(s1, 0)) * 100, 2) AS "growth_rate_1",
  ROUND(s3, 1) AS "sales_22Q4_23Q3_100M",
  ROUND(((s3 - s2) / NULLIF(s2, 0)) * 100, 2) AS "growth_rate_2",
  ROUND(s4, 1) AS "sales_23Q4_24Q3_100M",
  ROUND(((s4 - s3) / NULLIF(s3, 0)) * 100, 2) AS "growth_rate_3",
  ROUND(s5, 1) AS "sales_24Q4_25Q3_100M",
  ROUND(((s5 - s4) / NULLIF(s4, 0)) * 100, 2) AS "growth_rate_4"
FROM pivot_base
ORDER BY "ADSTRD_CD";

# %% [markdown]
# # ê²°ì¸¡ì¹˜ í™•ì¸
#  ë‘”ì´Œ1ë™ì´ nullë¡œ ë‚˜ì˜´ > ì¬ê±´ì¶•ì´ë¼ 2024Q4-2025Q3ë¶€í„° ë§¤ì¶œì´ ì¡íŒ ê±¸ í™•ì¸

# %%
SELECT * FROM sales_growth
WHERE "sales_20Q4_21Q3_100M" IS NULL 
   OR "sales_21Q4_22Q3_100M" IS NULL 
   OR "sales_22Q4_23Q3_100M" IS NULL 
   OR "sales_23Q4_24Q3_100M" IS NULL
   OR "sales_24Q4_25Q3_100M" IS NULL;

# %% [markdown]
# # new.csvë¥¼ supabaseì— ì—…ë¡œë“œí•˜ê¸°ì „ ì¸ì½”ë”© ë¬¸ì œ í•´ê²°í•˜ê¸°

# %%
import pandas as pd
# ë¶ˆëŸ¬ì˜¬ ë•Œ í•œê¸€ ì¸ì½”ë”© ì„¤ì • (cp949)
df = pd.read_csv('new.csv', encoding='cp949')
# ì €ì¥í•  ë•Œ UTF-8ë¡œ ì €ì¥
df.to_csv('new_utf8.csv', index=False, encoding='utf-8')

# %% [markdown]
# # íšŒê·€ë¶„ì„ 
# ì‹œì¥ í‰ê· ì„ ì„ ê·¸ë¦¬ê³  rising top 10ì„ ì°¾ê¸°

# %%
!uv add plotly

# %%
!uv add scikit-learn

# %%
!uv add nbformat

# %%
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.linear_model import LinearRegression

# 1. ë°ì´í„° ë¡œë“œ
df = pd.read_csv('seoul_4year_alpha_rows.csv')

# 2. íšŒê·€ë¶„ì„ (X: ìœ ë™ì¸êµ¬, y: ë§¤ì¶œ)
X = df[['pop_current_weight']].values
y = df['sales_4yr_legacy'].values
model = LinearRegression().fit(X, y)

df['predicted_sales'] = model.predict(X)
df['residual'] = df['sales_4yr_legacy'] - df['predicted_sales']

# 3. ì¸í„°ë™í‹°ë¸Œ ì‚°ì ë„ (Plotly)
fig = px.scatter(
    df, x='pop_current_weight', y='sales_4yr_legacy',
    hover_name='ADSTRD_CD_NM',
    color='efficiency_dna',
    size='efficiency_dna',
    color_continuous_scale='Portland',
    title='Seoul Commercial Alpha: 4yr Revenue Legacy vs Current Population Weight',
    labels={'pop_current_weight': 'Current Foot Traffic (Weight)', 'sales_4yr_legacy': '4yr Cumulative Sales (Legacy)'}
)

# íšŒê·€ì„ (ì‹œì¥ í‰ê·  ì„±ì¥ì„ ) ì¶”ê°€
x_range = np.linspace(df['pop_current_weight'].min(), df['pop_current_weight'].max(), 100)
y_range = model.predict(x_range.reshape(-1, 1))
fig.add_trace(go.Scatter(x=x_range, y=y_range, mode='lines', name='Market Average', line=dict(color='gray', dash='dash')))

# Rising Star Top 10 ê°•ì¡° (ì”ì°¨ ê¸°ì¤€)
top_10 = df.nlargest(10, 'residual')
for i, row in top_10.iterrows():
    fig.add_annotation(x=row['pop_current_weight'], y=row['sales_4yr_legacy'], text=row['ADSTRD_CD_NM'], showarrow=True, arrowhead=1)

fig.show()

# %%
!uv add statsmodels

# %%
import statsmodels.api as sm
import pandas as pd
import numpy as np

# 1. ë°ì´í„° ë¡œë“œ
df = pd.read_csv('seoul_4year_alpha_rows.csv')

# ë…ë¦½ë³€ìˆ˜(ì¸êµ¬)ì™€ ì¢…ì†ë³€ìˆ˜(ë§¤ì¶œ) ì¶”ì¶œ
X = df['pop_current_weight']
y = df['sales_4yr_legacy']

# ìƒìˆ˜í•­ ì¶”ê°€ (ì¤‘ìš”: statsmodelsëŠ” Interceptë¥¼ ìë™ìœ¼ë¡œ ë„£ì§€ ì•ŠìŒ)
X = sm.add_constant(X)

# ëª¨ë¸ ì í•©
results = sm.OLS(y, X).fit()

# ê²°ê³¼ ì¶œë ¥
print(results.summary())

# %%
import pandas as pd
import numpy as np
import statsmodels.api as sm

# 1. ë°ì´í„° ë¡œë“œ
df = pd.read_csv('seoul_4year_alpha_rows.csv')

# 2. ë¡œê·¸ ë³€í™˜ (ë°ì´í„°ê°€ 0ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ log(1+x) ì‚¬ìš©)
# ìƒê¶Œ ë°ì´í„°ì˜ ì™œë„(Skewness)ë¥¼ ì¤„ì—¬ í†µê³„ì  ìœ ì˜ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
df['log_pop'] = np.log1p(df['pop_current_weight'])
df['log_sales'] = np.log1p(df['sales_4yr_legacy'])

# 3. ë…ë¦½ë³€ìˆ˜(X)ì™€ ì¢…ì†ë³€ìˆ˜(y) ì„¤ì •
X = df['log_pop']
y = df['log_sales']
X = sm.add_constant(X)

# 4. íšŒê·€ë¶„ì„ ìˆ˜í–‰
model_log = sm.OLS(y, X).fit()

# 5. ê²°ê³¼ í™•ì¸
print(model_log.summary())

# %%
import pandas as pd
import plotly.express as px

df = pd.read_csv('seoul_4year_alpha_rows.csv')

# í‰ê·  ë§¤ì¶œ ì„±ì¥ë¥ ê³¼ í‰ê·  íš¨ìœ¨ì„± ê³„ì‚°
avg_sales = df['sales_4yr_legacy'].mean()
avg_efficiency = 1.0  # íˆ¬ì…=ì‚°ì¶œ ê¸°ì¤€ì 

fig = px.scatter(
    df, x='efficiency_dna', y='sales_4yr_legacy',
    hover_name='ADSTRD_CD_NM',
    color='efficiency_dna',
    size='sales_4yr_legacy',
    title="The Alpha Strategy: Efficiency vs Growth",
    labels={'efficiency_dna': 'Efficiency DNA (Quality)', 'sales_4yr_legacy': '4yr Sales Growth (Quantity)'}
)

# ê¸°ì¤€ì  ì‹­ìì„  ì¶”ê°€ (ì´ ì„ ë“¤ì´ ê°€ì„¤ì˜ ê¸°ì¤€ì´ ë©ë‹ˆë‹¤)
fig.add_vline(x=avg_efficiency, line_dash="dash", line_color="red", annotation_text="Efficiency Threshold")
fig.add_hline(y=avg_sales, line_dash="dash", line_color="blue", annotation_text="Avg Sales Growth")

fig.show()

# %%
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.linear_model import LinearRegression

# 1. ë°ì´í„° ë¡œë“œ
df = pd.read_csv('seoul_4year_alpha_rows.csv')

# 2. íšŒê·€ë¶„ì„ ìˆ˜í–‰
X = df[['pop_current_weight']].values
y = df['sales_4yr_legacy'].values
model = LinearRegression().fit(X, y)
df['predicted'] = model.predict(X)
df['residual'] = df['sales_4yr_legacy'] - df['predicted']

# 3. ì „ëµì  í•„í„°ë§: íšŒê·€ì„  ìœ„ì— ìˆëŠ”(ì”ì°¨ê°€ ì–‘ìˆ˜ì¸) ì§€ì—­ë§Œ 'ìœ íš¨'ë¡œ íŒì •
df['is_alpha'] = df['residual'] > 0

# 4. ì‹œê°í™” (íšŒê·€ì„  ì•„ë˜ ì§€ì—­ì€ íˆ¬ëª…í•˜ê²Œ ì²˜ë¦¬í•˜ê±°ë‚˜ 'Low Efficiency'ë¡œ ë¶„ë¥˜)
fig = px.scatter(
    df, 
    x='pop_current_weight', 
    y='sales_4yr_legacy',
    color='is_alpha', # íšŒê·€ì„  ìœ„/ì•„ë˜ êµ¬ë¶„
    hover_name='ADSTRD_CD_NM',
    size='efficiency_dna',
    title="<b>Seoul Alpha Analysis: Eliminating the 'Hype' Districts</b>",
    labels={'pop_current_weight': 'Population Weight', 'sales_4yr_legacy': '4yr Sales Legacy'},
    color_discrete_map={True: '#EF553B', False: '#E5ECF6'} # Alpha ì§€ì—­ì€ ë¹¨ê°•, ë¹„íš¨ìœ¨ì€ íšŒìƒ‰
)

# íšŒê·€ì„  ì¶”ê°€
x_range = np.linspace(df['pop_current_weight'].min(), df['pop_current_weight'].max(), 100)
y_range = model.predict(x_range.reshape(-1, 1))
import plotly.graph_objects as go
fig.add_trace(go.Scatter(x=x_range, y=y_range, mode='lines', name='Market Average Line', line=dict(color='black', dash='dot')))

fig.show()

# %%
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.linear_model import LinearRegression

# 1. ë°ì´í„° ë¡œë“œ
df = pd.read_csv('seoul_4year_alpha_rows.csv')

# 2. íšŒê·€ë¶„ì„ ë° ì”ì°¨(Residual) ê³„ì‚°
X = df[['pop_current_weight']].values
y = df['sales_4yr_legacy'].values
model = LinearRegression().fit(X, y)
df['predicted'] = model.predict(X)
df['residual'] = df['sales_4yr_legacy'] - df['predicted']

# 3. ì „ëµì  í•„í„°ë§ (íšŒê·€ì„  ìœ„ & Efficiency DNA ìƒìœ„ 10ê°œ)
df_alpha = df[df['residual'] > 0].copy()
top_10 = df_alpha.sort_values(by='efficiency_dna', ascending=False).head(10)

# 4. ì‹œê°í™” - ì „ì²´ ë°°ê²½ì€ ì—°í•˜ê²Œ, Top 10ì€ ê°•ë ¬í•˜ê²Œ
fig = px.scatter(
    df, x='pop_current_weight', y='sales_4yr_legacy',
    hover_name='ADSTRD_CD_NM',
    opacity=0.3,
    color_discrete_sequence=['gray'],
    title="<b>Seoul Rising Star Top 10: 'The Hidden Alpha'</b>",
    labels={'pop_current_weight': 'Current Population Weight', 'sales_4yr_legacy': '4yr Sales Legacy'}
)

# Top 10 ê°•ì¡° ë ˆì´ì–´ ì¶”ê°€
fig.add_trace(go.Scatter(
    x=top_10['pop_current_weight'],
    y=top_10['sales_4yr_legacy'],
    mode='markers+text',
    marker=dict(size=15, color='red', symbol='star'),
    text=top_10['ADSTRD_CD_NM'],
    textposition="top center",
    name='Top 10 Rising Stars'
))

# íšŒê·€ì„ (í‰ê· ì„ ) ì¶”ê°€
x_range = np.linspace(df['pop_current_weight'].min(), df['pop_current_weight'].max(), 100)
y_range = model.predict(x_range.reshape(-1, 1))
fig.add_trace(go.Scatter(x=x_range, y=y_range, mode='lines', name='Market Average', line=dict(color='black', dash='dot')))

fig.update_layout(template='plotly_white', showlegend=True)
fig.show()

# 5. ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
print("\n" + "="*50)
print("       [ì›”ìš”ì¼ ë°œí‘œìš© ìµœì¢… TOP 10 ë¦¬ìŠ¤íŠ¸]       ")
print("="*50)
print(top_10[['ADSTRD_CD_NM', 'sales_4yr_legacy', 'pop_current_weight', 'efficiency_dna']].to_string(index=False))
print("="*50)

# %%
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.linear_model import LinearRegression

# 1. ë°ì´í„° ë¡œë“œ ë° íšŒê·€ë¶„ì„
df = pd.read_csv('seoul_4year_alpha_rows.csv')
X = df[['pop_current_weight']].values
y = df['sales_4yr_legacy'].values
model = LinearRegression().fit(X, y)
df['predicted'] = model.predict(X)
df['residual'] = df['sales_4yr_legacy'] - df['predicted']

# 2. í•„í„°ë§: íšŒê·€ì„  ìœ„(Alpha > 0) ì§€ì—­ ì¤‘ íš¨ìœ¨ì„± ìƒìœ„ 20ê°œ
df_alpha = df[df['residual'] > 0].copy()
top_20 = df_alpha.sort_values(by='efficiency_dna', ascending=False).head(20).copy()

# 3. ê·¸ë£¹í•‘ (ì¸ì§€ë„ ê¸°ì¤€ ì„ì˜ ë¶„ë¥˜ - ë°œí‘œìš©)
# ë§¤ì¶œ ë°°ìˆ˜ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ 'ê¸°ì„± ìƒê¶Œ', ì ë‹¹íˆ ë†’ìœ¼ë©´ì„œ íš¨ìœ¨ì´ ê·¹ê°•ì´ë©´ 'ë¶ì—… ìƒê¶Œ'
top_20['type'] = top_20['sales_4yr_legacy'].apply(lambda x: 'Established' if x > 2.1 else 'Boom-up Candidate')

# 4. ì‹œê°í™”
fig = px.scatter(
    top_20, x='pop_current_weight', y='sales_4yr_legacy',
    color='type', size='efficiency_dna',
    hover_name='ADSTRD_CD_NM',
    text='ADSTRD_CD_NM',
    title="<b>Seoul Alpha Top 20: Hidden Gems Beyond the Majors</b>",
    labels={'pop_current_weight': 'Population Weight (Input)', 'sales_4yr_legacy': 'Sales Legacy (Output)'},
    color_discrete_map={'Established': '#636EFA', 'Boom-up Candidate': '#EF553B'}
)

fig.update_traces(textposition='top center')
fig.add_trace(go.Scatter(x=[0.8, 1.4], y=[model.predict([[0.8]])[0], model.predict([[1.4]])[0]], 
                         mode='lines', name='Market Avg', line=dict(color='black', dash='dot')))

fig.show()

# 5. ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
print(top_20[['ADSTRD_CD_NM', 'sales_4yr_legacy', 'pop_current_weight', 'efficiency_dna', 'type']].to_string(index=False))

# %%
import plotly.express as px
import plotly.graph_objects as go

# ì„±ìˆ˜ì˜ ìˆ˜ì¹˜ ì¶”ì¶œ (ì„±ìˆ˜2ê°€1ë™ ê¸°ì¤€)
seongsu = top_20[top_20['ADSTRD_CD_NM'] == 'ì„±ìˆ˜2ê°€1ë™'].iloc[0]
seongsu_dna = seongsu['efficiency_dna']

# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
fig = px.scatter(
    top_20, x='pop_current_weight', y='sales_4yr_legacy',
    color='efficiency_dna', size='efficiency_dna',
    hover_name='ADSTRD_CD_NM', text='ADSTRD_CD_NM',
    color_continuous_scale='Viridis',
    title="<b>Next Seongsu Analysis: Who beats Seongsu's Efficiency?</b>"
)

# ì„±ìˆ˜ ê¸°ì¤€ì„  (ì„¸ë¡œ/ê°€ë¡œì„  í˜¹ì€ ê°•ì¡° í‘œì‹œ)
fig.add_shape(type="circle",
    xref="x", yref="y",
    x0=seongsu['pop_current_weight']-0.02, y0=seongsu['sales_4yr_legacy']-0.05,
    x1=seongsu['pop_current_weight']+0.02, y1=seongsu['sales_4yr_legacy']+0.05,
    line_color="Red", line_width=3
)

fig.add_annotation(x=seongsu['pop_current_weight'], y=seongsu['sales_4yr_legacy'],
            text="BENCHMARK: SEONGSU", showarrow=True, arrowhead=1, ax=50, ay=-40, font=dict(color="red", size=12))

fig.update_layout(template='plotly_white')
fig.show()

# %% [markdown]
# # ê²°ë¡  - ë°ì´í„° ì…‹ ë¶€ì¡±
# í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ê°€ ì—†ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜´. ì›ì¸ì€ ë°ì´í„° ì…‹ ë¶€ì¡±ìœ¼ë¡œ ì¶©ë¶„íˆ ì„¤ëª…í•  ìˆ˜ ì—†ë‹¤ê³  íŒë‹¨ ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  í•„ìš”ê°€ ìˆìŒ

# %% [markdown]
# # ë°ì´í„° ë‹¤ì‹œ ìˆ˜ì§‘
# ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ ì„œë¹„ìŠ¤ : ìƒê¶Œë³€í™”ì§€í‘œ - í–‰ì •ë™
# ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ ì„œë¹„ìŠ¤ : ì§‘ê°ì‹œì„¤ - í–‰ì •ë™
# ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ ì„œë¹„ìŠ¤: ìƒì£¼ì¸êµ¬ - í–‰ì •ë™
# ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ ì„œë¹„ìŠ¤: ê¸¸ë‹¨ìœ„ì¸êµ¬ - í–‰ì •ë™
# ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ ì„œë¹„ìŠ¤: ì¶”ì •ë§¤ì¶œ - í–‰ì •ë™
# 
# ì—¬ê¸°ì„œ ê¸¸ë‹¨ìœ„ì¸êµ¬(ìœ ë™ì¸êµ¬) - í–‰ì •ë™ì„ ê¸°ì¤€ìœ¼ë¡œ í†µí•©í•˜ëŠ” ë°ì´í„° ì…‹ì„ ë§Œë“¤ ê²ƒì´ë‹¤.
# 
# 

# %%
import pandas as pd
import glob
import numpy as np
import os  # ë¡œì»¬ ê²½ë¡œ ì œì–´ë¥¼ ìœ„í•´ ì¶”ê°€

# [í™˜ê²½ì„¤ì •] ë°ì´í„° íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œë¡œ ì´ë™
# ì˜ˆ: r'C:\Users\Documents\Project\Data' (ê²½ë¡œ ì•ì— rì„ ë¶™ì´ë©´ ì—­ìŠ¬ë˜ì‹œ ì—ëŸ¬ ë°©ì§€)
data_path = r'C:\Users\Julia\Downloads\raw_data'
os.chdir(data_path)

# 1. ì„ ìƒë‹˜ì´ í™•ì •í•œ ì—…ì¢… ë° ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
target_sectors = [
    'ì„¬ìœ ì œí’ˆ', 'ì™„êµ¬', 'ìš´ë™/ê²½ê¸°_ìš©í’ˆ', 'í™”ì¥í’ˆ', 'ë¬¸êµ¬', 'ì„œì ', 
    'ì‹œê³„ë°ê·€ê¸ˆì†', 'ì•ˆê²½', 'ì¼ë°˜ì˜ë¥˜', 'í¸ì˜ì ', 'ë…¸ë˜ë°©', 'ë¯¸ìš©ì‹¤', 
    'ë‹¹êµ¬ì¥', 'ì»¤í”¼-ìŒë£Œ', 'í˜¸í”„-ê°„ì´ì£¼ì ', 'ë¶„ì‹ì „ë¬¸ì ', 'ì¹˜í‚¨ì „ë¬¸ì ', 
    'íŒ¨ìŠ¤íŠ¸í‘¸ë“œì ', 'ì œê³¼ì ', 'ì–‘ì‹ìŒì‹ì ', 'ì¤‘ì‹ìŒì‹ì ', 'í•œì‹ìŒì‹ì '
]

cols_to_keep = [
    'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ_ëª…', 'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…', 
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡', 'ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡', 
    'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡'
]

# 2. ì—°ë„ë³„ ë§¤ì¶œ ë°ì´í„° í†µí•© (ì¶”ì •ë§¤ì¶œë§Œ ì—°ë„ë³„)
print("ì§„í–‰ ì¤‘: ì—°ë„ë³„ ì¶”ì •ë§¤ì¶œ í†µí•©...")
sales_files = sorted(glob.glob('ë§¤ì¶œ_*.csv')) # íŒŒì¼ëª… ê·œì¹™ í™•ì¸ í•„ìš”
sales_list = []

for f in sales_files:
    # ë¶ˆëŸ¬ì˜¬ ë•Œë¶€í„° í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ì—¬ ë©”ëª¨ë¦¬ í™•ë³´
    df = pd.read_csv(f, usecols=cols_to_keep, encoding='cp949')
    # ì—…ì¢… í•„í„°ë§
    df = df[df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'].isin(target_sectors)]
    sales_list.append(df)

df_sales = pd.concat(sales_list, ignore_index=True)

# 3. ë‹¨ì¼ íŒŒì¼ ë°ì´í„° ë¡œë“œ (ìœ ë™ì¸êµ¬, ìƒì£¼ì¸êµ¬, ìƒê¶Œì§€í‘œ, ì§‘ê°ì‹œì„¤)
# ê° íŒŒì¼ì˜ ì»¬ëŸ¼ëª…ì€ ë°ì´í„° ê´‘ì¥ì˜ í‘œì¤€ëª…ì¹­ì„ ê¸°ì¤€ìœ¼ë¡œ í–ˆìŠµë‹ˆë‹¤.
print("ì§„í–‰ ì¤‘: ê¸°íƒ€ í…Œì´ë¸” ë³‘í•©...")

# [ì£¼ì˜] íŒŒì¼ëª…ì€ ë¡œì»¬ì— ì €ì¥ëœ ì´ë¦„ê³¼ ë˜‘ê°™ì•„ì•¼ í•©ë‹ˆë‹¤!
df_pop = pd.read_csv(r'C:\Users\Julia\Downloads\raw_data\ìœ ë™ì¸êµ¬.csv', encoding='cp949')
df_resident = pd.read_csv(r'C:\Users\Julia\Downloads\raw_data\ìƒì£¼ì¸êµ¬.csv', encoding='cp949')
df_change = pd.read_csv(r'C:\Users\Julia\Downloads\raw_data\ìƒê¶Œë³€í™”ì§€í‘œ.csv', encoding='cp949')
df_facility = pd.read_csv(r'C:\Users\Julia\Downloads\raw_data\ì§‘ê°ì‹œì„¤.csv', encoding='cp949')

# ê¸¸ë‹¨ìœ„ì¸êµ¬: MZ ìœ ë™ì¸êµ¬ í•µì‹¬
df_pop = df_pop[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜']]
df_pop['MZ_ìœ ë™ì¸êµ¬'] = df_pop['ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜'] + df_pop['ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜']

# ìƒì£¼ì¸êµ¬: ë² ë“œíƒ€ìš´ ì§€ìˆ˜ìš© 
df_resident = df_resident[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì´_ê°€êµ¬_ìˆ˜']]

# ì§‘ê°ì‹œì„¤: ì¸í”„ë¼ ìœ„ì£¼
df_facility = df_facility[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ', 'ì§‘ê°ì‹œì„¤_ìˆ˜', 'ì§€í•˜ì² _ì—­_ìˆ˜']]

# ìƒê¶Œë³€í™”ì§€í‘œ: ì—­ë™ì„± ìŠ¤ì½”ì–´ë§ í¬í•¨
df_change = df_change[['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ', 'ìƒê¶Œ_ë³€í™”_ì§€í‘œ_ëª…', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ']]
# ì•„ê¹Œ ì •í•œ 1~4ì  ë§¤í•‘ ì ìš©
mapping = {'ë‹¤ì´ë‚˜ë¯¹': 4, 'ìƒê¶Œí™•ì¥': 3, 'ì •ì²´': 2, 'ìƒê¶Œì¶•ì†Œ': 1}
df_change['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] = df_change['ìƒê¶Œ_ë³€í™”_ì§€í‘œ_ëª…'].map(mapping).fillna(0)

# 4. ìµœì¢… Merge (ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œì™€ í–‰ì •ë™_ì½”ë“œë¥¼ í‚¤ë¡œ í™œìš©)
final_df = df_sales.merge(df_pop, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ'], how='left')
final_df = final_df.merge(df_resident, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ'], how='left')
final_df = final_df.merge(df_change, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ'], how='left')
final_df = final_df.merge(df_facility, on=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ'], how='left')

# 5. ê²°ê³¼ ì €ì¥
final_df.fillna(0, inplace=True)
final_df.to_csv('final_alpha_data.csv', index=False, encoding='utf-8-sig')
print("ì¶•í•˜í•©ë‹ˆë‹¤! ë¶„ì„ìš© ìµœì¢… ë°ì´í„° ì…‹ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# %%
# 1. í–‰ ê°œìˆ˜ ë¹„êµ
original_rows = len(df_sales)
final_rows = len(final_df)

print(f"--- [1. ë°ì´í„° ì†ì‹¤ ê²€ì¦] ---")
print(f"ë§¤ì¶œ ë°ì´í„° ì›ë³¸ í–‰ ìˆ˜: {original_rows}")
print(f"ìµœì¢… ë³‘í•© ë°ì´í„° í–‰ ìˆ˜: {final_rows}")

if original_rows == final_rows:
    print("âœ… ì„±ê³µ: ë°ì´í„° ëˆ„ë½ì´ë‚˜ ì¤‘ë³µ ìƒì„± ì—†ì´ ì™„ë²½í•˜ê²Œ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("âš ï¸ ì£¼ì˜: í–‰ ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ì¤‘ë³µ ë°ì´í„°(Duplication)ê°€ ìˆëŠ”ì§€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

# %%
print(f"\n--- [2. ì£¼ìš” ì§€í‘œ ê²°ì¸¡ì¹˜(0) ë¹„ì¤‘ ê²€ì¦] ---")
# ì£¼ìš” ì»¬ëŸ¼ë“¤ ë¦¬ìŠ¤íŠ¸ (ì„ ìƒë‹˜ íŒŒì¼ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ìˆ˜ì • í•„ìš”)
check_cols = ['ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜', 'ì§‘ê°ì‹œì„¤ìˆ˜']

for col in check_cols:
    if col in final_df.columns:
        # 0ê°’ì˜ ë¹„ì¤‘ ê³„ì‚°
        zero_count = (final_df[col] == 0).sum()
        zero_ratio = (zero_count / len(final_df)) * 100
        print(f"[{col}] ê²°ì¸¡ì¹˜(0) ê°œìˆ˜: {zero_count}ê°œ ({zero_ratio:.2f}%)")

# %%
# 1. ë°ì´í„° ìƒë‹¨ ë° êµ¬ì¡° í™•ì¸
print("--- [1. ë°ì´í„° ê¸°ë³¸ êµ¬ì¡°] ---")
print(final_df.info()) 

# 2. ìš”ì•½ í†µê³„ëŸ‰ í™•ì¸ (ë§¤ì¶œ, ì¸êµ¬, ì§€ìˆ˜ ë“±ì´ ìƒì‹ì ì¸ ë²”ìœ„ì¸ì§€)
print("\n--- [2. ì£¼ìš” ì§€í‘œ ìš”ì•½ í†µê³„] ---")
# ë¶„ì„ì— í•µì‹¬ì ì¸ ì»¬ëŸ¼ë“¤ë§Œ ê³¨ë¼ì„œ ë´…ë‹ˆë‹¤.
key_cols = ['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜', 'MZ_ìœ ë™ì¸êµ¬', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']
# ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§í•´ì„œ í™•ì¸
existing_cols = [c for c in key_cols if c in final_df.columns]
print(final_df[existing_cols].describe())

# 3. ë°ì´í„° ì¤‘ë³µ ì—¬ë¶€ í™•ì¸
# ë™ì¼ ë¶„ê¸°ì— ë™ì¼ í–‰ì •ë™, ë™ì¼ ì—…ì¢…ì´ ë‘ ë²ˆ ë“¤ì–´ê°€ë©´ ì•ˆ ë©ë‹ˆë‹¤.
duplicate_count = final_df.duplicated(subset=['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ', 'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…']).sum()
print(f"\n--- [3. ì¤‘ë³µ ë°ì´í„° ì²´í¬] ---")
print(f"ì¤‘ë³µëœ í–‰(Row) ê°œìˆ˜: {duplicate_count}ê°œ")

# 4. 'ê°€ì–‘ë™' vs 'ì„±ìˆ˜ë™' ê·¹ëª…í•œ ì°¨ì´ í™•ì¸ (Spot Check)
print(f"\n--- [4. ë² ë“œíƒ€ìš´ vs í•«í”Œë ˆì´ìŠ¤ ë¹„êµ ê²€ì¦] ---")
comparison = final_df[final_df['í–‰ì •ë™_ì½”ë“œ_ëª…'].isin(['ê°€ì–‘1ë™', 'ì„±ìˆ˜2ê°€1ë™'])].groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')[existing_cols].mean()
print(comparison)

# %%
# 1. íš¨ìœ¨ ì§€í‘œ ìƒì„±
# ë°°í›„ ì¸êµ¬(ìƒì£¼ì¸êµ¬) ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ì™¸ë¶€ì—ì„œ ë§ì´ ì˜¤ë‚˜? (ìƒê¶Œ íš¨ìœ¨ì„±)
final_df['ìƒê¶Œ_ìœ ì…_ê°•ë„'] = final_df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'] / (final_df['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'] + 1)

# 2. MZ íƒ€ê²ŸíŒ… ì§€í‘œ
# ì „ì²´ ìœ ë™ì¸êµ¬ ì¤‘ MZ(2030)ê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨
final_df['MZ_ìœ ì…_ë¹„ì¤‘'] = (final_df['ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜'] + final_df['ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜']) / (final_df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'] + 1)

# 3. ë°ì´í„° í¬ì¸íŠ¸ ìµœì‹ í™” (2025ë…„ 1ë¶„ê¸° ê¸°ì¤€)
analysis_2025 = final_df[final_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'] == 20251].copy()

# %%
# í–‰ì •ë™ë³„ë¡œ ì§€í‘œ í†µí•©
dong_rank = analysis_2025.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').agg({
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ìƒê¶Œ_ìœ ì…_ê°•ë„': 'mean',
    'MZ_ìœ ì…_ë¹„ì¤‘': 'mean',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean'
}).reset_index()

# [í•„í„°ë§ ì¡°ê±´]
# 1. ìƒê¶Œì§€í‘œ ì ìˆ˜ê°€ 2.5ì  ì´ìƒì¸ ê³³ (ì •ì²´ëœ ë² ë“œíƒ€ìš´ ì œê±°)
# 2. MZ ìœ ì… ë¹„ì¤‘ì´ ì„œìš¸ì‹œ í‰ê·  ì´ìƒì¸ ê³³
df_filtered = dong_rank[
    (dong_rank['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] >= 2.5) & 
    (dong_rank['MZ_ìœ ì…_ë¹„ì¤‘'] >= dong_rank['MZ_ìœ ì…_ë¹„ì¤‘'].mean())
]

# ìµœì¢… 'ì„±ìˆ˜ ì§€ìˆ˜' ì‚°ì¶œ (ìœ ì… ê°•ë„ì™€ MZ ë¹„ì¤‘ì˜ ì¡°í™”)
df_filtered['Next_Seongsu_Score'] = (df_filtered['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.5) + (df_filtered['MZ_ìœ ì…_ë¹„ì¤‘'] * 100 * 0.5)

# ìƒìœ„ 10ê°œ ì¶œë ¥
top_10 = df_filtered.sort_values(by='Next_Seongsu_Score', ascending=False).head(10)
print(top_10[['í–‰ì •ë™_ì½”ë“œ_ëª…', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'MZ_ìœ ì…_ë¹„ì¤‘', 'Next_Seongsu_Score']])

# %%
import statsmodels.api as sm
import numpy as np

# 1. ë¶„ì„ìš© ë°ì´í„° ì •ë¦¬ (ìµœì‹  ë¶„ê¸° ê¸°ì¤€)
df_reg = analysis_2025.copy()

# 2. ë³€ìˆ˜ ìŠ¤ì¼€ì¼ ì¡°ì • (ë§¤ì¶œì•¡ ë‹¨ìœ„ê°€ ë„ˆë¬´ í¬ë¯€ë¡œ ë¡œê·¸ë¥¼ ì·¨í•˜ê±°ë‚˜ ë‹¨ìœ„ë¥¼ ì–µìœ¼ë¡œ ë³€ê²½)
df_reg['ë§¤ì¶œ_ì–µë‹¨ìœ„'] = df_reg['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / 100000000

# 3. ë…ë¦½ë³€ìˆ˜(X)ì™€ ì¢…ì†ë³€ìˆ˜(y) ì„¤ì •
# ìƒìˆ˜í•­(Intercept) ì¶”ê°€ í•„ìˆ˜
X = df_reg[['MZ_ìœ ë™ì¸êµ¬', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜']]
X = sm.add_constant(X) 
y = df_reg['ë§¤ì¶œ_ì–µë‹¨ìœ„']

# 4. ëª¨ë¸ í•™ìŠµ ë° ê²°ê³¼ ì¶œë ¥
model = sm.OLS(y, X).fit()
print(model.summary())

# %%
# ë‹¨ìœ„ ì¡°ì • (ê°€ë…ì„± ë° í†µê³„ì  ì•ˆì •ì„± í–¥ìƒ)
df_reg['MZ_ìœ ë™_ì²œëª…'] = df_reg['MZ_ìœ ë™ì¸êµ¬'] / 1000
df_reg['ìƒì£¼ì¸êµ¬_ì²œëª…'] = df_reg['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'] / 1000

# ë‹¤ì‹œ ë¶„ì„
X_new = df_reg[['MZ_ìœ ë™_ì²œëª…', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜', 'ìƒì£¼ì¸êµ¬_ì²œëª…']]
X_new = sm.add_constant(X_new)
model_new = sm.OLS(y, X_new).fit()
print(model_new.summary())

# %%
!uv pip install matplotlib seaborn

# %%
import matplotlib.pyplot as plt
import seaborn as sns

# íšŒê·€ê³„ìˆ˜ ë°ì´í„° ì¶”ì¶œ (ìƒìˆ˜í•­ ì œì™¸)
coefs = model_new.params.drop('const')
errors = model_new.bse.drop('const')

# ì‹œê°í™” ì„¤ì •
plt.figure(figsize=(10, 6))
plt.rc('font', family='Malgun Gothic') # í•œê¸€ ê¹¨ì§ ë°©ì§€
plt.axvline(0, color='red', linestyle='--') # 0ì  ê¸°ì¤€ì„ 
coefs.plot(kind='barh', xerr=errors, color='skyblue', edgecolor='black')
plt.title('ìƒê¶Œ ë§¤ì¶œì— ë¯¸ì¹˜ëŠ” ë³€ìˆ˜ë³„ ì˜í–¥ë ¥ (íšŒê·€ê³„ìˆ˜)')
plt.xlabel('ë§¤ì¶œ ê¸°ì—¬ë„ (ì–µ ë‹¨ìœ„)')
plt.show()

# %%
import statsmodels.api as sm
import numpy as np

# 1. ì „ì²´ ë°ì´í„° ë³µì‚¬ ë° í´ë¦¬ë‹
df_total_reg = final_df.copy()

# 2. ë‹¨ìœ„ ì¡°ì • (ë§¤ì¶œì€ ì–µ ë‹¨ìœ„, ì¸êµ¬ëŠ” ì²œ ëª… ë‹¨ìœ„)
df_total_reg['ë§¤ì¶œ_ì–µë‹¨ìœ„'] = df_total_reg['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / 100000000
df_total_reg['MZ_ìœ ë™_ì²œëª…'] = (df_total_reg['ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜'] + df_total_reg['ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜']) / 1000
df_total_reg['ìƒì£¼ì¸êµ¬_ì²œëª…'] = df_total_reg['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'] / 1000
# ìƒê¶Œ_ìœ ì…_ê°•ë„ ì¬ê³„ì‚° (ì›ë³¸ì— ì—†ë‹¤ë©´)
df_total_reg['ìƒê¶Œ_ìœ ì…_ê°•ë„'] = df_total_reg['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'] / (df_total_reg['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'] + 1)

# 3. ê²°ì¸¡ì¹˜ ì œê±° (ì „ì²´ ë°ì´í„°ëŠ” ì–‘ì´ ë§ì•„ ê²°ì¸¡ì¹˜ê°€ ì„ì—¬ìˆì„ í™•ë¥ ì´ ë†’ìŒ)
cols_to_use = ['MZ_ìœ ë™_ì²œëª…', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜', 'ìƒì£¼ì¸êµ¬_ì²œëª…']
df_total_reg = df_total_reg.dropna(subset=cols_to_use + ['ë§¤ì¶œ_ì–µë‹¨ìœ„'])

# 4. ë…ë¦½ë³€ìˆ˜(X)ì™€ ì¢…ì†ë³€ìˆ˜(y) ì„¤ì •
X_total = df_total_reg[cols_to_use]
X_total = sm.add_constant(X_total)
y_total = df_total_reg['ë§¤ì¶œ_ì–µë‹¨ìœ„']

# 5. ëª¨ë¸ í•™ìŠµ ë° ê²°ê³¼ ì¶œë ¥
model_total = sm.OLS(y_total, X_total).fit()
print(model_total.summary())

# %%
# ì½”ë¡œë‚˜ ê¸°ê°„(2020~2022)ë§Œ ë”°ë¡œ ë–¼ì–´ë‚´ê¸°
corona_df = final_df[final_df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].between(20201, 20224)].copy()

# ë‹¨ìœ„ ì¡°ì •
corona_df['ë§¤ì¶œ_ì–µë‹¨ìœ„'] = corona_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] / 100000000
corona_df['MZ_ìœ ë™_ì²œëª…'] = (corona_df['ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜'] + corona_df['ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜']) / 1000
corona_df['ìƒì£¼ì¸êµ¬_ì²œëª…'] = corona_df['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'] / 1000
corona_df['ìƒê¶Œ_ìœ ì…_ê°•ë„'] = corona_df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜'] / (corona_df['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'] + 1)

# íšŒê·€ë¶„ì„ ì‹¤í–‰
X_corona = corona_df[['MZ_ìœ ë™_ì²œëª…', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜', 'ìƒì£¼ì¸êµ¬_ì²œëª…']]
X_corona = sm.add_constant(X_corona)
y_corona = corona_df['ë§¤ì¶œ_ì–µë‹¨ìœ„']

model_corona = sm.OLS(y_corona, X_corona).fit()
print(model_corona.summary())

# %%
# 1. ë™ë„¤(í–‰ì •ë™)ë³„ë¡œ ë°ì´í„° í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±° ë° í‰ê· ê°’ ì‚°ì¶œ)
final_agg = analysis_2025.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').agg({
    'ìƒê¶Œ_ìœ ì…_ê°•ë„': 'mean',
    'MZ_ìœ ì…_ë¹„ì¤‘': 'mean',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean'
}).reset_index()

# 2. ë‹¤ì‹œ ì •ê·œí™” (0~1 ì‚¬ì´ë¡œ ë³€í™˜)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
final_agg[['ìƒê¶Œ_ìœ ì…_ê°•ë„', 'MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']] = scaler.fit_transform(
    final_agg[['ìƒê¶Œ_ìœ ì…_ê°•ë„', 'MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']]
)

# 3. ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ì§€ìˆ˜ ì‚°ì¶œ (ê°€ì¤‘ì¹˜ ì ìš©)
# MZ ë¹„ì¤‘ì— ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì¤ë‹ˆë‹¤.
final_agg['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    final_agg['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.4 + 
    final_agg['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.3 + 
    final_agg['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.3
) * 100

# 4. ê²°ê³¼ ì¶œë ¥ (TOP 10)
result = final_agg.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False).head(10)
print(result[['í–‰ì •ë™_ì½”ë“œ_ëª…', 'ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', 'MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„']])

# %%
# 1. ë™ë„¤ë³„ ë§¤ì¶œ ê·œëª¨ íŒŒì•… (ì´ë¯¸ ë„ˆë¬´ í° ê³³ì„ ë¹¼ê¸° ìœ„í•´)
final_agg_with_sales = analysis_2025.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').agg({
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ìƒê¶Œ_ìœ ì…_ê°•ë„': 'mean',
    'MZ_ìœ ì…_ë¹„ì¤‘': 'mean',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean'
}).reset_index()

# 2. ìƒìœ„ 20% ë§¤ì¶œ ìƒê¶Œ(ì´ë¯¸ ë©”ì´ì €ì¸ ê³³) ì œì™¸
sales_threshold = final_agg_with_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].quantile(0.8)
next_candidates = final_agg_with_sales[final_agg_with_sales['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] < sales_threshold].copy()

# 3. ì •ê·œí™” ë° ì§€ìˆ˜ ì¬ê³„ì‚°
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
next_candidates[['ìƒê¶Œ_ìœ ì…_ê°•ë„', 'MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']] = scaler.fit_transform(
    next_candidates[['ìƒê¶Œ_ìœ ì…_ê°•ë„', 'MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']]
)

# 4. ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ì§€ìˆ˜ (ì„±ì¥ì„±ì— ë” ë¹„ì¤‘)
next_candidates['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    next_candidates['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.5 +  # MZê°€ ëª¨ì´ëŠ” ê²Œ ì œì¼ ì¤‘ìš”
    next_candidates['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.3 + 
    next_candidates['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.2
) * 100

# 5. ìµœì¢… ê²°ê³¼ (TOP 10)
final_next_seongsu = next_candidates.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False).head(10)
print(final_next_seongsu[['í–‰ì •ë™_ì½”ë“œ_ëª…', 'ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', 'MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„']])

# %%
# 1. ì™¸ë¶€ ìœ ì…ì´ ë„ˆë¬´ ì ì€ ê³³(ë‹¨ìˆœ ìì·¨ì´Œ) í•„í„°ë§
# ìƒê¶Œ ìœ ì… ê°•ë„ê°€ í•˜ìœ„ 30%ì¸ ê³³ì€ ê³¼ê°íˆ ì œê±° (ë†€ëŸ¬ ì˜¤ëŠ” ì‚¬ëŒì´ ì ë‹¤ëŠ” ëœ»)
inflow_threshold = next_candidates['ìƒê¶Œ_ìœ ì…_ê°•ë„'].quantile(0.3)
final_hip_candidates = next_candidates[next_candidates['ìƒê¶Œ_ìœ ì…_ê°•ë„'] > inflow_threshold].copy()

# 2. ì§€ìˆ˜ ì¬ì‚°ì¶œ (ê°€ì¤‘ì¹˜ ì¡°ì •)
# 'ìœ ì… ê°•ë„' ë¹„ì¤‘ì„ ë†’ì—¬ì„œ 'ë†€ëŸ¬ ì˜¤ëŠ” ê³³'ì— ê°€ì ì„ ì¤ë‹ˆë‹¤.
final_hip_candidates['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    final_hip_candidates['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.4 + 
    final_hip_candidates['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.4 + # ì™¸ë¶€ ìœ ì…ì˜ ì¤‘ìš”ë„ ìƒìŠ¹
    final_hip_candidates['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.2
) * 100

# 3. ìµœì¢… ìˆœìœ„ í™•ì¸
result_final = final_hip_candidates.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False).head(10)
print(result_final[['í–‰ì •ë™_ì½”ë“œ_ëª…', 'ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', 'MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„']])

# %%
# 1. MZ ìœ ë™ì¸êµ¬ í•©ê³„ ê³„ì‚°
analysis_2025['MZ_ìœ ë™_ìˆ˜'] = analysis_2025['ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜'] + analysis_2025['ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜']

# 2. í˜„ì‹¤ì ì¸ í•„í„°ë§ (ìì·¨ì´Œ & ì¬ê±´ì¶•ì§€ì—­ ì œì™¸)
# MZ ìœ ë™ì¸êµ¬ê°€ ë„ˆë¬´ ì ì€ ê³³(í•˜ìœ„ 20%) ì œì™¸ = "ì‚¬ëŒì´ ì¼ë‹¨ ëª¨ì—¬ì•¼ ìƒê¶Œì´ë‹¤"
mz_threshold = analysis_2025['MZ_ìœ ë™_ìˆ˜'].quantile(0.2)
# ìƒì£¼ì¸êµ¬ê°€ ë„ˆë¬´ ì ì€ ê³³(í•˜ìœ„ 10%) ì œì™¸ = "ìœ ì…ê°•ë„ ìˆ˜ì¹˜ ì™œê³¡ ë°©ì§€"
resident_threshold = analysis_2025['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'].quantile(0.1)

valid_df = analysis_2025[
    (analysis_2025['MZ_ìœ ë™_ìˆ˜'] > mz_threshold) & 
    (analysis_2025['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'] > resident_threshold)
].copy()

# 3. ì´ë¯¸ ë„ˆë¬´ ì»¤ì§„ ë©”ì´ì € ìƒê¶Œ ì œì™¸ (ìƒìœ„ 20% ë§¤ì¶œì•¡)
sales_limit = valid_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].quantile(0.8)
next_step_df = valid_df[valid_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] < sales_limit].copy()

# 4. ë™ë„¤ë³„ í‰ê· ê°’ìœ¼ë¡œ í•©ì¹˜ê¸°
final_real = next_step_df.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').agg({
    'MZ_ìœ ì…_ë¹„ì¤‘': 'mean',
    'ìƒê¶Œ_ìœ ì…_ê°•ë„': 'mean',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean'
}).reset_index()

# 5. ì •ê·œí™” ë° ì§€ìˆ˜ ì‚°ì¶œ (4:4:2 ê°€ì¤‘ì¹˜)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
final_real[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']] = scaler.fit_transform(
    final_real[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']]
)

final_real['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    final_real['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.4 + 
    final_real['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.4 + 
    final_real['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.2
) * 100

print(final_real.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False).head(10))

# %%
# 1. í˜„ì‹¤ì ì¸ ì²´ê¸‰ í•„í„° (ìì·¨ì´Œ & ì¬ê±´ì¶• ë°©ì§€ - ì´ì „ê³¼ ë™ì¼)
valid_df = analysis_2025[
    (analysis_2025['MZ_ìœ ë™_ìˆ˜'] > analysis_2025['MZ_ìœ ë™_ìˆ˜'].quantile(0.2)) & 
    (analysis_2025['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'] > analysis_2025['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜'].quantile(0.1))
].copy()

# 2. ê°•ë ¥í•œ ë§¤ì¶œ í•„í„°: ìƒìœ„ 50% ë™ë„¤ë¥¼ í†µì§¸ë¡œ ì œê±°
# ì„œìš¸ì—ì„œ ë§¤ì¶œ ê·œëª¨ê°€ ì¤‘ê°„ ì´í•˜ì¸ 'ì„±ì¥ê¸°' ë™ë„¤ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
mid_low_sales_limit = valid_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].quantile(0.5)
emerging_df = valid_df[valid_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] < mid_low_sales_limit].copy()

# 3. ë™ë„¤ë³„ í‰ê· ê°’ ì§‘ê³„
final_agg_next = emerging_df.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').agg({
    'MZ_ìœ ì…_ë¹„ì¤‘': 'mean',
    'ìƒê¶Œ_ìœ ì…_ê°•ë„': 'mean',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean'
}).reset_index()

# 4. ì •ê·œí™” (ë‚¨ì€ ë™ë„¤ë“¤ ì‚¬ì´ì—ì„œì˜ ìƒëŒ€ ì ìˆ˜)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
final_agg_next[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']] = scaler.fit_transform(
    final_agg_next[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']]
)

# 5. ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ì§€ìˆ˜ ì‚°ì¶œ (MZ 40% : ìœ ì…ê°•ë„ 40% : ì§€í‘œ 20%)
final_agg_next['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    final_agg_next['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.4 + 
    final_agg_next['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.4 + 
    final_agg_next['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.2
) * 100

print(final_agg_next.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False).head(15))

# %%
# 1. 'ìƒì£¼ì¸êµ¬ ëŒ€ë¹„ ì™¸ë¶€ ìœ ì…'ì´ ì„œìš¸ í‰ê·  ì´ìƒì¸ ê³³ë§Œ ë‚¨ê¸°ê¸° (ìì·¨ì´Œ í•„í„°)
# ìœ ì… ê°•ë„ê°€ ë‚®ë‹¤ëŠ” ê±´ 'ê·¸ ë™ë„¤ ì‚¬ëŠ” ì‚¬ëŒ' ìœ„ì£¼ë¼ëŠ” ëœ»ì´ë¯€ë¡œ ê³¼ê°íˆ ì œê±°
inflow_mean = valid_df['ìƒê¶Œ_ìœ ì…_ê°•ë„'].mean()
hip_only_df = valid_df[valid_df['ìƒê¶Œ_ìœ ì…_ê°•ë„'] > inflow_mean].copy()

# 2. ì´ë¯¸ ë„ˆë¬´ ëœ¬ ë©”ì´ì € ìƒê¶Œ ì œì™¸ (ì•„ê¹Œë³´ë‹¤ ë” ê°•ë ¥í•˜ê²Œ 50% ì»·)
sales_median = hip_only_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].median()
emerging_hip_df = hip_only_df[hip_only_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] < sales_median].copy()

# 3. ë™ë„¤ë³„ í‰ê·  ì§‘ê³„
final_agg_final = emerging_hip_df.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').agg({
    'MZ_ìœ ì…_ë¹„ì¤‘': 'mean',
    'ìƒê¶Œ_ìœ ì…_ê°•ë„': 'mean',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean'
}).reset_index()

# 4. ì •ê·œí™”
scaler = MinMaxScaler()
final_agg_final[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']] = scaler.fit_transform(
    final_agg_final[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']]
)

# 5. [ì¤‘ìš”] ê°€ì¤‘ì¹˜ ë³€ê²½: ìœ ì… ê°•ë„(ì™¸ë¶€ì—ì„œ ì˜¤ëŠ” í˜)ë¥¼ 50%ë¡œ ìƒí–¥
final_agg_final['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    final_agg_final['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.5 +  # ì™¸ë¶€ ì§‘ê°ë ¥ì´ ê°€ì¥ ì¤‘ìš”!
    final_agg_final['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.3 + 
    final_agg_final['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.2
) * 100

print(final_agg_final.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False).head(10))

# %%
# 1. 'ì´ë¯¸ ì™„ì„±ëœ' ìƒê¶Œì§€í‘œ 4ì (ë§Œì ) ì§€ì—­ ì œì™¸
# 'ì„±ì¥ê¸°' í˜¹ì€ 'í™•ì¥ê¸°'ì¸ 2~3ì ëŒ€ ì§€ì—­ë§Œ íƒ€ê²ŸíŒ…í•©ë‹ˆë‹¤.
emerging_stage_df = valid_df[valid_df['ìƒê¶Œì§€í‘œ_ì ìˆ˜'].isin([2, 3])].copy()

# 2. ë§¤ì¶œ í•„í„° (ì¤‘ê°„ ì´í•˜)
sales_limit = emerging_stage_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'].median()
target_df = emerging_stage_df[emerging_stage_df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] < sales_limit].copy()

# 3. ë™ë„¤ë³„ ì§‘ê³„
final_agg_next = target_df.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').agg({
    'MZ_ìœ ì…_ë¹„ì¤‘': 'mean',
    'ìƒê¶Œ_ìœ ì…_ê°•ë„': 'mean',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean'
}).reset_index()

# 4. ì •ê·œí™”
scaler = MinMaxScaler()
final_agg_next[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']] = scaler.fit_transform(
    final_agg_next[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']]
)

# 5. ê°€ì¤‘ì¹˜: ì™¸ë¶€ ìœ ì…(0.5) + MZ ë¹„ì¤‘(0.3) + ì§€í‘œ ì„±ì¥ì„±(0.2)
final_agg_next['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    final_agg_next['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.5 + 
    final_agg_next['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.3 + 
    final_agg_next['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.2
) * 100

print(final_agg_next.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False).head(10))

# %%
# 1. ì „ ê¸°ê°„(df_total)ì— ëŒ€í•´ í–‰ì •ë™ë³„ í‰ê·  ì§€í‘œ ì‚°ì¶œ
# (ì—°ë„ë³„ ë³€í™”ë¥¼ ë³´ê¸° ìœ„í•´ groupbyì— 'ì—°ë„'ë¥¼ í¬í•¨í•˜ê±°ë‚˜, ì „ì²´ í‰ê· ì„ ëƒ…ë‹ˆë‹¤)
all_time_agg = final_df.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').agg({
    'MZ_ìœ ì…_ë¹„ì¤‘': 'mean',
    'ìƒê¶Œ_ìœ ì…_ê°•ë„': 'mean',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean',
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'mean'
}).reset_index()

# 2. ì „ì²´ ê¸°ê°„ ê¸°ì¤€ ì •ê·œí™”
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
all_time_agg[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']] = scaler.fit_transform(
    all_time_agg[['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']]
)

# 3. ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ì§€ìˆ˜ ì‚°ì¶œ (4:4:2)
all_time_agg['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    all_time_agg['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.4 + 
    all_time_agg['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.4 + 
    all_time_agg['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.2
) * 100

# 4. ì „ì²´ ê¸°ê°„ í†µí•© ë­í‚¹ TOP 20
total_ranking = all_time_agg.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False)
print(total_ranking[['í–‰ì •ë™_ì½”ë“œ_ëª…', 'ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', 'MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„']].head(20))

# %%
import matplotlib.pyplot as plt
import seaborn as sns

# í•œê¸€ í°íŠ¸ ì„¤ì • (í™˜ê²½ì— ë”°ë¼ 'NanumBarunGothic' ë˜ëŠ” 'Malgun Gothic' ì‚¬ìš©)
plt.rc('font', family='NanumBarunGothic') 

plt.figure(figsize=(14, 9))

# 1. íšŒê·€ì„ ì´ í¬í•¨ëœ ì‚°ì ë„ (weight ëŒ€ì‹  linewidth ì‚¬ìš©)
sns.regplot(x='ìƒê¶Œ_ìœ ì…_ê°•ë„', y='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', data=all_time_agg, 
            scatter_kws={'alpha':0.4, 'color':'gray', 's':50}, 
            line_kws={'color':'#e74c3c', 'linewidth':3}) # ì—¬ê¸°ì„œ ìˆ˜ì •ë¨

# 2. ì£¼ìš” ì§€ì—­(TOP 15)ì— ë¼ë²¨ë§ (ìˆ˜ìœ , ë²ˆë™, ì„œêµ ë“±)
# ì ë“¤ì´ ê²¹ì¹˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•´ ì•½ê°„ì˜ ì˜¤í”„ì…‹ì„ ì¤ë‹ˆë‹¤.
top_15 = all_time_agg.sort_values(by='ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False).head(15)

for i, row in top_15.iterrows():
    plt.text(row['ìƒê¶Œ_ìœ ì…_ê°•ë„'] + 0.005, row['ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'], row['í–‰ì •ë™_ì½”ë“œ_ëª…'], 
             fontsize=11, fontweight='bold', va='center', alpha=0.9)

# 3. ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ë§
plt.title('ìƒê¶Œ ìœ ì… ê°•ë„(ë…ë¦½ë³€ìˆ˜)ì™€ ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ì§€ìˆ˜(ì¢…ì†ë³€ìˆ˜)ì˜ ìƒê´€ê´€ê³„', fontsize=18, pad=20)
plt.xlabel('ìƒê¶Œ ìœ ì… ê°•ë„ (ì™¸ë¶€ ì§‘ê°ë ¥)', fontsize=13)
plt.ylabel('ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ì§€ìˆ˜ (ì„±ì¥ ì ì¬ë ¥)', fontsize=13)
plt.axvline(all_time_agg['ìƒê¶Œ_ìœ ì…_ê°•ë„'].mean(), color='blue', linestyle='--', alpha=0.3) # í‰ê· ì„  ì¶”ê°€
plt.grid(True, linestyle=':', alpha=0.6)

plt.tight_layout()
plt.show()

# %%
# 1. ë…ë¦½ë³€ìˆ˜ ì •ê·œí™” (Min-Max Scaling)
# ì´ì œ 'ì§‘ê°ì‹œì„¤_ì´_ìˆ˜'ê°€ ìˆ˜ìœ ë™ì˜ ì¼ì‹œì  ë…¸ì´ì¦ˆë¥¼ ëˆ„ë¥´ëŠ” 'ì•ˆì „ì¥ì¹˜' ì—­í• ì„ í•©ë‹ˆë‹¤.
cols_to_scale = ['MZ_ìœ ì…_ë¹„ì¤‘', 'ìƒê¶Œ_ìœ ì…_ê°•ë„', 'ì§‘ê°ì‹œì„¤_ì´_ìˆ˜', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']
final_df[cols_to_scale] = scaler.fit_transform(final_df[cols_to_scale])

# 2. ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ì§€ìˆ˜ 4.0 (ê°€ì¤‘ì¹˜ ì¡°ì •)
# ì‹œì„¤ì´ ë¶€ì¡±í•œë° ìœ ì…ë§Œ ë§ì€ ì§€ì—­ì„ ê±¸ëŸ¬ë‚´ê¸° ìœ„í•´ 'ì§‘ê°ì‹œì„¤'ì— í˜ì„ ì¤ë‹ˆë‹¤.
final_df['ìµœì¢…_ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜'] = (
    final_df['ì§‘ê°ì‹œì„¤_ì´_ìˆ˜'] * 0.3 +   # ìƒê¶Œì˜ ë¬¼ë¦¬ì  ê¸°ì´ˆ (Hard)
    final_df['ìƒê¶Œ_ìœ ì…_ê°•ë„'] * 0.3 +   # ì™¸ë¶€ ì§‘ê° ë™ë ¥ (Dynamic)
    final_df['MZ_ìœ ì…_ë¹„ì¤‘'] * 0.3 +      # ìˆ˜ìš”ì˜ ì„±ê²© (Soft)
    final_df['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.1      # ë³€í™”ì˜ ì†ë„
) * 100

# 3. ë­í‚¹ ì¬ì‚°ì¶œ
final_ranking = final_df.sort_values(by='ìµœì¢…_ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', ascending=False)
print(final_ranking[['í–‰ì •ë™_ì½”ë“œ_ëª…', 'ìµœì¢…_ë„¥ìŠ¤íŠ¸_ì„±ìˆ˜_ì§€ìˆ˜', 'ì§‘ê°ì‹œì„¤_ì´_ìˆ˜', 'ìƒê¶Œ_ìœ ì…_ê°•ë„']].head(15))

# %% [markdown]
# # ì§€ìˆ˜ë¥¼ ë‹¤ì‹œ ë§Œë“¤ì–´ì„œ íšŒê·€ë¥¼ ì‹œì¼œë´„.

# %%
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import re

# 1. ë¶„ì„ìš© ë³µì‚¬ë³¸ ìƒì„± (ì›ë³¸ ë³´ì¡´)
df_analysis = final_df.copy()

# 2. í–‰ì •ë™ ëª…ì¹­ í†µí•© (ì¢…ë¡œ1234ê°€ë™ ë“± ìœ ë ¹ë¬¸ì ì œê±°)
df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'] = df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'].str.replace(r'[^ê°€-í£0-9]', '', regex=True)

# 3. [í•µì‹¬] ì´ë¦„ì´ í†µí•©ëœ ë™ë„¤ë“¤ì˜ ì›ì²œ ë°ì´í„° í•©ì‚° (ì¤‘ìš”!)
# ë¹„ì¤‘(%)ì„ í‰ê·  ë‚´ëŠ” ê²Œ ì•„ë‹ˆë¼, ì „ì²´ í•©ê³„ë¥¼ êµ¬í•œ ë’¤ ë‚˜ì¤‘ì— ë‹¤ì‹œ ê³„ì‚°í•´ì•¼ ì •í™•í•©ë‹ˆë‹¤.
df_grouped = df_analysis.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ_ëª…']).agg({
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'MZ_ìœ ë™ì¸êµ¬': 'sum',
    'ì´_ìœ ë™ì¸êµ¬_ìˆ˜': 'sum',
    'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì§‘ê°ì‹œì„¤_ìˆ˜': 'max',          # ì‹œì„¤ì€ í•©ì¹˜ëŠ” ê²Œ ì•„ë‹ˆë¼ í•´ë‹¹ ì§€ì—­ì˜ ê·œëª¨ì„
    'ì§€í•˜ì² _ì—­_ìˆ˜': 'max',          # ì—­ ìˆ˜ë„ ë§ˆì°¬ê°€ì§€
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean',
    'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ': 'mean',
    'ì´_ìœ ë™ì¸êµ¬_ìˆ˜': 'sum'         # ìœ ì… ê°•ë„ ëŒ€ìš© ì§€í‘œ
}).reset_index()

# 4. [í•µì‹¬] íšŒê·€ì‹ì— ë“¤ì–´ê°ˆ ë…ë¦½ë³€ìˆ˜(X) ì¬ì‚°ì¶œ
# í•©ì³ì§„ ì´í•©ì„ ê¸°ì¤€ìœ¼ë¡œ ë¹„ì¤‘ì„ êµ¬í•´ì•¼ 'ì¢…ë¡œ'ì˜ ì§„ì§œ íŒŒê´´ë ¥ì´ ë‚˜ì˜µë‹ˆë‹¤.
df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] = df_grouped['MZ_ìœ ë™ì¸êµ¬'] / df_grouped['ì´_ìœ ë™ì¸êµ¬_ìˆ˜']
df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] = (df_grouped['ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡'] + df_grouped['ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡']) / df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']

# 5. ì •ê·œí™” (Min-Max Scaling)
scaler = MinMaxScaler()
# íšŒê·€ì‹ì— ì“°ì¼ 6ê°œ ë…ë¦½ë³€ìˆ˜ ì„ ì •
cols = ['ì§‘ê°ì‹œì„¤_ìˆ˜', 'ì§€í•˜ì² _ì—­_ìˆ˜', 'MZ_ìœ ë™_ë¹„ì¤‘', 'MZ_ë§¤ì¶œ_ë¹„ì¤‘', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']
df_grouped[cols] = scaler.fit_transform(df_grouped[cols])

# 6. [íšŒê·€ ìˆ˜ì‹ ë°˜ì˜] ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ìµœì¢… ì ìˆ˜ ì‚°ì¶œ
# ì¸í”„ë¼(35%) + MZì˜í–¥ë ¥(40%) + ì§€ì†ì„±(25%)
df_grouped['ìµœì¢…_ì§€ìˆ˜'] = (
    (df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜'] * 0.25 + df_grouped['ì§€í•˜ì² _ì—­_ìˆ˜'] * 0.10) +  # Infra
    (df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] * 0.15 + df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] * 0.25) +  # MZ Power
    (df_grouped['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.15 + df_grouped['ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· '] * 0.10) # Dynamics
) * 100

# 7. í–‰ì •ë™ë³„ ì „ ê¸°ê°„ í‰ê·  ë­í‚¹ ì¶œë ¥
final_ranking = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['ìµœì¢…_ì§€ìˆ˜'].mean().sort_values(ascending=False)
print("--- [ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ì§€ìˆ˜] ë°ì´í„° í´ë¦¬ë‹ ë° ìˆ˜ì‹ ë°˜ì˜ ìµœì¢… ê²°ê³¼ ---")
print(final_ranking.head(15))

# %%
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import re

# 1. ë¶„ì„ìš© ë³µì‚¬ë³¸ ìƒì„±
df_analysis = final_df.copy()

# 2. í–‰ì •ë™ ëª…ì¹­ í†µí•© (ì„±ìˆ˜ë™ ì „ì²´ + ì¢…ë¡œ ì „ì²´)
# 'ì„±ìˆ˜'ê°€ ë“¤ì–´ê°€ë©´ ë¬´ì¡°ê±´ 'ì„±ìˆ˜ë™_í†µí•©', 'ì¢…ë¡œ'ê°€ ë“¤ì–´ê°€ë©´ 'ì¢…ë¡œ_í†µí•©'
def unify_name(name):
    name = re.sub(r'[^ê°€-í£0-9]', '', name)
    if 'ì„±ìˆ˜' in name: return 'ì„±ìˆ˜ë™_í†µí•©'
    if 'ì¢…ë¡œ' in name: return 'ì¢…ë¡œ_í†µí•©'
    return name

df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'] = df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'].apply(unify_name)

# 3. ë°ì´í„° ì§‘ê³„ (ë¹„ì¤‘ ì¬ì‚°ì¶œì„ ìœ„í•´ ì›ì²œ ë°ì´í„° sum)
df_grouped = df_analysis.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ_ëª…']).agg({
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'MZ_ìœ ë™ì¸êµ¬': 'sum',
    'ì´_ìœ ë™ì¸êµ¬_ìˆ˜': 'sum',
    'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì§‘ê°ì‹œì„¤_ìˆ˜': 'max', 
    'ì§€í•˜ì² _ì—­_ìˆ˜': 'max',
    'ìƒê¶Œì§€í‘œ_ì ìˆ˜': 'mean',
    'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ': 'mean'
}).reset_index()

# 4. ê°€ì¤‘ì¹˜ ë³€ìˆ˜ ì¬ê³„ì‚°
df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] = df_grouped['MZ_ìœ ë™ì¸êµ¬'] / df_grouped['ì´_ìœ ë™ì¸êµ¬_ìˆ˜']
df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] = (df_grouped['ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡'] + df_grouped['ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡']) / df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']

# 5. ì •ê·œí™” ë° ìµœì¢… ì§€ìˆ˜ ì‚°ì¶œ
scaler = MinMaxScaler()
cols = ['ì§‘ê°ì‹œì„¤_ìˆ˜', 'ì§€í•˜ì² _ì—­_ìˆ˜', 'MZ_ìœ ë™_ë¹„ì¤‘', 'MZ_ë§¤ì¶œ_ë¹„ì¤‘', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜']
df_grouped[cols] = scaler.fit_transform(df_grouped[cols])

# íšŒê·€ ìˆ˜ì‹ ê°€ì¤‘ì¹˜ ì ìš©
df_grouped['ìµœì¢…_ì§€ìˆ˜'] = (
    (df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜'] * 0.25 + df_grouped['ì§€í•˜ì² _ì—­_ìˆ˜'] * 0.10) +
    (df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] * 0.15 + df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] * 0.25) +
    (df_grouped['ìƒê¶Œì§€í‘œ_ì ìˆ˜'] * 0.15 + df_grouped['ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· '] * 0.10)
) * 100

# 6. ìµœì¢… ë­í‚¹ í™•ì¸
final_ranking = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['ìµœì¢…_ì§€ìˆ˜'].mean().sort_values(ascending=False)
print(final_ranking.head(20))

# %%
# ì„±ìˆ˜ë™_í†µí•©ì˜ ê°œë³„ ì§€í‘œ ì ìˆ˜ í™•ì¸
seongsu_score = df_grouped[df_grouped['í–‰ì •ë™_ì½”ë“œ_ëª…'] == 'ì„±ìˆ˜ë™_í†µí•©']
print("--- [ì„±ìˆ˜ë™_í†µí•©] ìƒì„¸ ì„±ì í‘œ ---")
print(seongsu_score[['MZ_ìœ ë™_ë¹„ì¤‘', 'MZ_ë§¤ì¶œ_ë¹„ì¤‘', 'ì§‘ê°ì‹œì„¤_ìˆ˜', 'ìƒê¶Œì§€í‘œ_ì ìˆ˜', 'ìµœì¢…_ì§€ìˆ˜']])

# ì „ì²´ ìˆœìœ„ì—ì„œ ëª‡ ìœ„ì¸ì§€ í™•ì¸
all_ranks = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['ìµœì¢…_ì§€ìˆ˜'].mean().sort_values(ascending=False).reset_index()
print(f"\nì„±ìˆ˜ë™ì˜ í˜„ì¬ ìˆœìœ„: {all_ranks[all_ranks['í–‰ì •ë™_ì½”ë“œ_ëª…'] == 'ì„±ìˆ˜ë™_í†µí•©'].index[0] + 1}ìœ„")

# %%
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import re

# 1. ë¶„ì„ìš© ë³µì‚¬ë³¸ ìƒì„±
df_analysis = final_df.copy()

# 2. í–‰ì •ë™ ëª…ì¹­ ì „ì²˜ë¦¬ (ì„±ìˆ˜ë§Œ í†µí•©)
# ì¢…ë¡œëŠ” ì´ë¯¸ 'ì¢…ë¡œ1234ê°€ë™'ìœ¼ë¡œ ë¬¶ì—¬ ìˆìœ¼ë‹ˆ íŠ¹ìˆ˜ë¬¸ìë§Œ ì •ë¦¬í•˜ë©´ ë©ë‹ˆë‹¤.
def unify_names(name):
    # íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì œê±°
    clean_name = re.sub(r'[^ê°€-í£0-9]', '', name)
    # ì„±ìˆ˜ë™ ì‹œë¦¬ì¦ˆëŠ” í•˜ë‚˜ë¡œ í†µí•©
    if 'ì„±ìˆ˜' in clean_name:
        return 'ì„±ìˆ˜ë™_í†µí•©'
    return clean_name

df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'] = df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'].apply(unify_names)

# 3. ë°ì´í„° ê·¸ë£¹í™” (ì‹¤ì¸¡ ë°ì´í„° ê¸°ë°˜ í•©ì‚°)
df_grouped = df_analysis.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ_ëª…']).agg({
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'MZ_ìœ ë™ì¸êµ¬': 'sum',
    'ì´_ìœ ë™ì¸êµ¬_ìˆ˜': 'sum',
    'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì§‘ê°ì‹œì„¤_ìˆ˜': 'max', 
    'ì§€í•˜ì² _ì—­_ìˆ˜': 'max',
    'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ': 'mean'
}).reset_index()

# 4. ê°€ì¤‘ì¹˜ ë³€ìˆ˜ ì¬ê³„ì‚° (ë¹„ì¤‘ ë° ìƒê¶Œ ì—ë„ˆì§€)
# 'ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜'ëŠ” (ë§¤ì¶œì•¡ * ì‹œì„¤ìˆ˜)ë¡œ ê³„ì‚°í•˜ì—¬ ë…¼ë°­(ì‹œì„¤ìˆ˜ 0ì— ê°€ê¹Œì›€)ì„ ì›ì²œ ì°¨ë‹¨í•©ë‹ˆë‹¤.
df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] = df_grouped['MZ_ìœ ë™ì¸êµ¬'] / df_grouped['ì´_ìœ ë™ì¸êµ¬_ìˆ˜']
df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] = (df_grouped['ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡'] + df_grouped['ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡']) / df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']
df_grouped['ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜'] = df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] * df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜']

# 5. ì •ê·œí™”
scaler = MinMaxScaler()
cols = ['ì§‘ê°ì‹œì„¤_ìˆ˜', 'ì§€í•˜ì² _ì—­_ìˆ˜', 'MZ_ìœ ë™_ë¹„ì¤‘', 'MZ_ë§¤ì¶œ_ë¹„ì¤‘', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜']
df_grouped[cols] = scaler.fit_transform(df_grouped[cols])

# 6. ìµœì¢… ë„¥ìŠ¤íŠ¸ ì„±ìˆ˜ ìˆ˜ì‹ (NSI) ì ìš©
df_grouped['ìµœì¢…_ì§€ìˆ˜'] = (
    (df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜'] * 0.25 + df_grouped['ì§€í•˜ì² _ì—­_ìˆ˜'] * 0.10) + # ì¸í”„ë¼(35%)
    (df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] * 0.35 + df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] * 0.10) + # MZíŒŒì›Œ(45%)
    (df_grouped['ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜'] * 0.10 + df_grouped['ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· '] * 0.10) # ì§€ì†/ì—ë„ˆì§€(20%)
) * 100

# 7. ìµœì¢… ë­í‚¹
final_ranking = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['ìµœì¢…_ì§€ìˆ˜'].mean().sort_values(ascending=False)
print(final_ranking.head(20))

# %%
# 1. í–‰ì •ë™ë³„ë¡œ ì „ ê¸°ê°„ í‰ê·  ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ìˆœìœ„ ë§¤ê¸°ê¸°
final_ranking_df = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['ìµœì¢…_ì§€ìˆ˜'].mean().sort_values(ascending=False).reset_index()
final_ranking_df.index = final_ranking_df.index + 1  # ìˆœìœ„ë¥¼ 1ë¶€í„° ì‹œì‘í•˜ê²Œ ë³€ê²½
final_ranking_df.columns = ['í–‰ì •ë™_ì½”ë“œ_ëª…', 'NSI_ì§€ìˆ˜']

# 2. ìƒìœ„ 30ìœ„ ì¶œë ¥
print("--- [NSI 8.0] NEXT ì„±ìˆ˜ ì§€ìˆ˜ ìƒìœ„ 30ìœ„ ---")
print(final_ranking_df.head(30))

print("\n" + "="*50)

# 3. 'ì„±ìˆ˜ë™_í†µí•©'ì´ ëª‡ ìœ„ì— ìˆëŠ”ì§€ ì°¾ì•„ë‚´ê¸°
try:
    seongsu_rank = final_ranking_df[final_ranking_df['í–‰ì •ë™_ì½”ë“œ_ëª…'] == 'ì„±ìˆ˜ë™_í†µí•©'].index[0]
    seongsu_score = final_ranking_df.loc[seongsu_rank, 'NSI_ì§€ìˆ˜']
    print(f"â˜… ì„±ìˆ˜ë™_í†µí•©ì˜ í˜„ì¬ ìœ„ì¹˜: {seongsu_rank}ìœ„ (ì ìˆ˜: {seongsu_score:.2f}ì )")
    
    # ì„±ìˆ˜ë™ ì•ë’¤ ìˆœìœ„ í™•ì¸ (ì„±ìˆ˜ê°€ ì™œ ë°€ë ¸ëŠ”ì§€ ë¹„êµìš©)
    print(f"\n--- ì„±ìˆ˜ë™ ì¸ê·¼ ìˆœìœ„ (Rank {max(1, seongsu_rank-2)} ~ {seongsu_rank+2}) ---")
    print(final_ranking_df.iloc[max(0, seongsu_rank-3):seongsu_rank+2])
    
except IndexError:
    print("ì„±ìˆ˜ë™_í†µí•© ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ì— ì—†ìŠµë‹ˆë‹¤. í†µí•© ë¡œì§ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")

# %%
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import re

# 1. ë¶„ì„ìš© ë³µì‚¬ë³¸ ìƒì„± (ì´ë²ˆì—ëŠ” ì„±ìˆ˜ í†µí•© ì•ˆ í•¨!)
df_analysis = final_df.copy()

# 2. ëª…ì¹­ ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±°)
df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'] = df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'].str.replace(r'[^ê°€-í£0-9]', '', regex=True)

# 3. ë°ì´í„° ê·¸ë£¹í™” (í–‰ì •ë™ë³„ ê°œë³„ ì§‘ê³„)
df_grouped = df_analysis.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ_ëª…']).agg({
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'MZ_ìœ ë™ì¸êµ¬': 'sum',
    'ì´_ìœ ë™ì¸êµ¬_ìˆ˜': 'sum',
    'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì§‘ê°ì‹œì„¤_ìˆ˜': 'max', 
    'ì§€í•˜ì² _ì—­_ìˆ˜': 'max',
    'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ': 'mean'
}).reset_index()

# 4. ê°€ì¤‘ì¹˜ ë³€ìˆ˜ ì¬ê³„ì‚°
df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] = df_grouped['MZ_ìœ ë™ì¸êµ¬'] / df_grouped['ì´_ìœ ë™ì¸êµ¬_ìˆ˜']
df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] = (df_grouped['ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡'] + df_grouped['ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡']) / df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']
df_grouped['ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜'] = df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] * df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜']

# 5. ì •ê·œí™” ë° NSI 8.0 ìˆ˜ì‹ ì ìš© (ì´ì „ê³¼ ë™ì¼ ê°€ì¤‘ì¹˜)
scaler = MinMaxScaler()
cols = ['ì§‘ê°ì‹œì„¤_ìˆ˜', 'ì§€í•˜ì² _ì—­_ìˆ˜', 'MZ_ìœ ë™_ë¹„ì¤‘', 'MZ_ë§¤ì¶œ_ë¹„ì¤‘', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜']
df_grouped[cols] = scaler.fit_transform(df_grouped[cols])

df_grouped['ìµœì¢…_ì§€ìˆ˜'] = (
    (df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜'] * 0.25 + df_grouped['ì§€í•˜ì² _ì—­_ìˆ˜'] * 0.10) +
    (df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] * 0.35 + df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] * 0.10) +
    (df_grouped['ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜'] * 0.10 + df_grouped['ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· '] * 0.10)
) * 100

# 6. ìµœì¢… ë­í‚¹ í™•ì¸
final_ranking = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['ìµœì¢…_ì§€ìˆ˜'].mean().sort_values(ascending=False).reset_index()
final_ranking.index = final_ranking.index + 1
print(final_ranking.head(101)) # 101ìœ„ ë¦¬ìŠ¤íŠ¸ í™•ì¸

# %%
import statsmodels.api as sm

# 1. ê²€ì¦ì— ì‚¬ìš©í•  ë³€ìˆ˜ë“¤ ì„ íƒ (ìš°ë¦¬ê°€ ìˆ˜ì‹ì— ì“´ ë…ë¦½ë³€ìˆ˜ë“¤)
features = ['ì§‘ê°ì‹œì„¤_ìˆ˜', 'ì§€í•˜ì² _ì—­_ìˆ˜', 'MZ_ìœ ë™_ë¹„ì¤‘', 'MZ_ë§¤ì¶œ_ë¹„ì¤‘', 'ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ']
X = df_grouped[features]
y = df_grouped['ìµœì¢…_ì§€ìˆ˜']

# 2. ìƒìˆ˜í•­(Intercept) ì¶”ê°€
X = sm.add_constant(X)

# 3. OLS(Ordinary Least Squares) ëª¨ë¸ í”¼íŒ…
model = sm.OLS(y, X).fit()

# 4. ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥
print("--- [NSI 8.0] íšŒê·€ ëª¨ë¸ í†µê³„ì  ê²€ì¦ ê²°ê³¼ ---")
print(model.summary())

# %%
# 1. ì „ì²´ í–‰ì •ë™ í‰ê·  ì ìˆ˜ ê³„ì‚° ë° ìˆœìœ„ ìƒì„±
# ë¶„ê¸°ë³„ ë°ì´í„°ë¥¼ í–‰ì •ë™ë³„ í‰ê· ìœ¼ë¡œ ìš”ì•½
df_final_avg = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…').mean().reset_index()

# 2. ì „ì²´ ìˆœìœ„ ë§¤ê¸°ê¸° (ìµœì¢…_ì§€ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
df_final_avg = df_final_avg.sort_values(ascending=False, by='ìµœì¢…_ì§€ìˆ˜').reset_index(drop=True)
df_final_avg['ìˆœìœ„'] = df_final_avg.index + 1  # 1ìœ„ë¶€í„° ì‹œì‘í•˜ëŠ” ìˆœìœ„ ì»¬ëŸ¼ ì¶”ê°€

# 3. ì„±ìˆ˜ê°€ í¬í•¨ëœ í–‰ì •ë™ë§Œ í•„í„°ë§
seongsu_results = df_final_avg[df_final_avg['í–‰ì •ë™_ì½”ë“œ_ëª…'].str.contains('ì„±ìˆ˜')].copy()

# 4. ê²°ê³¼ ì¶œë ¥ (ì£¼ìš” ì§€í‘œ í¬í•¨)
print("--- [ì„±ìˆ˜ë™ 4ê°œ í–‰ì •ë™ ì •ë°€ ë¶„ì„ ê²°ê³¼] ---")
output_cols = ['ìˆœìœ„', 'í–‰ì •ë™_ì½”ë“œ_ëª…', 'ìµœì¢…_ì§€ìˆ˜', 'MZ_ë§¤ì¶œ_ë¹„ì¤‘', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'ì§‘ê°ì‹œì„¤_ìˆ˜']
print(seongsu_results[output_cols])


# %%
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import re

# 1. ë¶„ì„ìš© ë³µì‚¬ë³¸ ìƒì„± (ì´ë²ˆì—ëŠ” ì„±ìˆ˜ í†µí•© ì•ˆ í•¨!)
df_analysis = final_df.copy()

# 2. ëª…ì¹­ ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±°)
df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'] = df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'].str.replace(r'[^ê°€-í£0-9]', '', regex=True)

# 3. ë°ì´í„° ê·¸ë£¹í™” (í–‰ì •ë™ë³„ ê°œë³„ ì§‘ê³„)
df_grouped = df_analysis.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ_ëª…']).agg({
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'MZ_ìœ ë™ì¸êµ¬': 'sum',
    'ì´_ìœ ë™ì¸êµ¬_ìˆ˜': 'sum',
    'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì§‘ê°ì‹œì„¤_ìˆ˜': 'max', 
    'ì§€í•˜ì² _ì—­_ìˆ˜': 'max',
    'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ': 'mean'
}).reset_index()

# 4. ê°€ì¤‘ì¹˜ ë³€ìˆ˜ ì¬ê³„ì‚°
df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] = df_grouped['MZ_ìœ ë™ì¸êµ¬'] / df_grouped['ì´_ìœ ë™ì¸êµ¬_ìˆ˜']
df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] = (df_grouped['ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡'] + df_grouped['ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡']) / df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡']
df_grouped['ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜'] = df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] * df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜']

# 5. ì •ê·œí™” ë° NSI 8.0 ìˆ˜ì‹ ì ìš© (ì´ì „ê³¼ ë™ì¼ ê°€ì¤‘ì¹˜)
scaler = MinMaxScaler()
cols = ['ì§‘ê°ì‹œì„¤_ìˆ˜', 'ì§€í•˜ì² _ì—­_ìˆ˜', 'MZ_ìœ ë™_ë¹„ì¤‘', 'MZ_ë§¤ì¶œ_ë¹„ì¤‘', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜']
df_grouped[cols] = scaler.fit_transform(df_grouped[cols])

df_grouped['ìµœì¢…_ì§€ìˆ˜'] = (
    (df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜'] * 0.25 + df_grouped['ì§€í•˜ì² _ì—­_ìˆ˜'] * 0.10) +
    (df_grouped['MZ_ë§¤ì¶œ_ë¹„ì¤‘'] * 0.35 + df_grouped['MZ_ìœ ë™_ë¹„ì¤‘'] * 0.10) +
    (df_grouped['ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜'] * 0.10 + df_grouped['ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· '] * 0.10)
) * 100

# 6. ìµœì¢… ë­í‚¹ ìƒì„± (ê¸°ì¡´ ì½”ë“œ ì—°ì¥)
final_ranking = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['ìµœì¢…_ì§€ìˆ˜'].mean().sort_values(ascending=False).reset_index()
final_ranking.columns = ['í–‰ì •ë™', 'ìµœì¢…_ì§€ìˆ˜'] # ì»¬ëŸ¼ëª… ì •ë¦¬
final_ranking.insert(0, 'Rank', range(1, len(final_ranking) + 1)) # ìˆœìœ„ ì‚½ì…

# 7. ìƒìœ„ 101ìœ„ë§Œ ì¶”ì¶œ
top_101 = final_ranking.head(101).copy()

# 4. CSVë¡œ ì €ì¥ (í•œê¸€ ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ utf-8-sig ì‚¬ìš©)
top_101.to_csv('final_ranking_101.csv', index=False, encoding='utf-8-sig')

print("âœ… csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì—‘ì…€ì—ì„œ 'final_ranking_101.csv'ë¥¼ ì—¬ì„¸ìš”!")


# %%
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.preprocessing import MinMaxScaler

# 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬
df_analysis = final_df.copy()
df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'] = df_analysis['í–‰ì •ë™_ì½”ë“œ_ëª…'].str.replace(r'[^ê°€-í£0-9]', '', regex=True)

# 2. í–‰ì •ë™ë³„/ë¶„ê¸°ë³„ ê·¸ë£¹í™”
df_grouped = df_analysis.groupby(['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'í–‰ì •ë™_ì½”ë“œ_ëª…']).agg({
    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'MZ_ìœ ë™ì¸êµ¬': 'sum',
    'ì´_ìœ ë™ì¸êµ¬_ìˆ˜': 'sum',
    'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡': 'sum',
    'ì§‘ê°ì‹œì„¤_ìˆ˜': 'max', 
    'ì§€í•˜ì² _ì—­_ìˆ˜': 'max',
    'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ': 'mean'
}).reset_index().sort_values(['í–‰ì •ë™_ì½”ë“œ_ëª…', 'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'])

# 3. íƒ€ê²Ÿ ë³€ìˆ˜: MZ ìœ ë™ì¸êµ¬ ì¦ê°€ìœ¨
df_grouped['MZ_ìœ ë™_ì¦ê°€ìœ¨'] = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['MZ_ìœ ë™ì¸êµ¬'].pct_change()
df_grouped = df_grouped.replace([np.inf, -np.inf], np.nan).dropna(subset=['MZ_ìœ ë™_ì¦ê°€ìœ¨'])

# 4. ë…ë¦½ë³€ìˆ˜ ì¬êµ¬ì„± (MZ ê´€ë ¨ ëª¨ë“  ë³€ìˆ˜ ì œì™¸)
# ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜ ê³„ì‚° ì‹œ 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'ì€ ì „ì²´ ë§¤ì¶œì´ë¯€ë¡œ ìœ ì§€
df_grouped['ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜'] = df_grouped['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'] * df_grouped['ì§‘ê°ì‹œì„¤_ìˆ˜']

# 5. ì •ê·œí™” ë° ìµœì¢… ë³€ìˆ˜ ì„ ì •
scaler = MinMaxScaler()
# ìˆœìˆ˜ ìƒê¶Œ í™˜ê²½ ë³€ìˆ˜ë“¤ë¡œë§Œ êµ¬ì„±
cols = ['ì§‘ê°ì‹œì„¤_ìˆ˜', 'ì§€í•˜ì² _ì—­_ìˆ˜', 'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'ìƒê¶Œ_ì—ë„ˆì§€_ì§€ìˆ˜']
df_grouped[cols] = scaler.fit_transform(df_grouped[cols])

# 6. OLS íšŒê·€ë¶„ì„ ì‹¤í–‰
X = df_grouped[cols]
X = sm.add_constant(X)
Y = df_grouped['MZ_ìœ ë™_ì¦ê°€ìœ¨']

model = sm.OLS(Y, X).fit()
weights = model.params

# 7. ìµœì¢… ì§€ìˆ˜ ì‚°ì¶œ (í†µê³„ ê°€ì¤‘ì¹˜ ë°˜ì˜)
df_grouped['ìµœì¢…_ì§€ìˆ˜'] = sum(df_grouped[col] * weights[col] for col in cols)

# 8. ìµœì¢… ë­í‚¹ ë° 101ìœ„ ë¦¬ìŠ¤íŠ¸ì—…
# ì „ì²´ ê¸°ê°„ì— ëŒ€í•œ í–‰ì •ë™ë³„ í‰ê·  ì ìˆ˜ë¡œ ë­í‚¹ ì‚°ì •
final_ranking = df_grouped.groupby('í–‰ì •ë™_ì½”ë“œ_ëª…')['ìµœì¢…_ì§€ìˆ˜'].mean().sort_values(ascending=False).reset_index()
final_ranking.insert(0, 'Rank', range(1, len(final_ranking) + 1))

# CSV ì €ì¥ (ì´ì œ ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë“¤ê³  ë„¤ì´ë²„ íŠ¸ë Œë“œë¡œ ê°€ì„œ ê²€ì¦í•˜ì‹œë©´ ë©ë‹ˆë‹¤!)
final_ranking.head(50).to_csv('NSI_9.0_Candidate_List.csv', index=False, encoding='utf-8-sig')

# ì¡°ì›ë“¤ì—ê²Œ ë³´ì—¬ì¤„ í†µê³„ ê·¼ê±° ì¶œë ¥
print(model.summary())


