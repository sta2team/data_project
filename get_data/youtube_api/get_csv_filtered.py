import requests
from dotenv import load_dotenv
import os
import json
import csv
from datetime import datetime
import config as cf

load_dotenv()
API_KEY = os.getenv("YOUTUBE_API")

# @@@@@@@ config.pyì—ì„œ ì„¤ì • @@@@@@@

# ë‹¨ì¼ í‚¤ì›Œë“œ ì„¤ì • 
# keyword = "ê°€ì‚° ì¹´í˜"

print(f"\n{'='*50}")
print(f"ğŸ“‹ ìƒì„±ëœ í‚¤ì›Œë“œ: {len(cf.KEYWORDS)}ê°œ") 
for i, kw in enumerate(cf.KEYWORDS, 1): 
    print(f"  {i}. {kw}") 
print(f"{'='*50}\n")

def search_youtube(query, publishedAfter, publishedBefore, total_count=100, max_results=50, order="relevance", region_code="KR"):
    """
    YouTube ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸°

    Args:
        query (str): ê²€ìƒ‰ í‚¤ì›Œë“œ
        publishedAfter (str): ê²€ìƒ‰ ì‹œì‘ ë‚ ì§œ
        publishedBefore (str): ê²€ìƒ‰ ì¢…ë£Œ ë‚ ì§œ
        total_count (int): ì´ ê°€ì ¸ì˜¬ ê²°ê³¼ ìˆ˜
        max_results (int): í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê²°ê³¼ ìˆ˜ (0-50)
        order (str): ì •ë ¬ ë°©ì‹ (date, rating, relevance, title, videoCount, viewCount)
        region_code (str): ì§€ì—­ ì½”ë“œ

    Returns:
        tuple: (video_ids ë¦¬ìŠ¤íŠ¸, all_items ë¦¬ìŠ¤íŠ¸, ê²€ìƒ‰ íŒŒë¼ë¯¸í„° dict, page_info dict)
    """

    base_url = "https://www.googleapis.com/youtube/v3/search"

    video_ids = []
    all_items = []
    next_page_token = None
    total_fetched = 0
    total_results_from_api = 0
    results_per_page_from_api = 0

    search_params = {
        "query": query,
        "publishedAfter": cf.publishedAfter,
        "publishedBefore": cf.publishedBefore,
        "order": cf.order,
        "regionCode": region_code,
        "requestedCount": total_count,
        "maxResultsPerPage": max_results
    }

    print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {query}")
    print(f"ê¸°ê°„: {cf.publishedAfter} ~ {cf.publishedBefore}")
    print("-" * 50)

    while total_fetched < total_count:
        params = {
            "key": API_KEY,
            "part": "snippet",
            "q": query,
            "publishedAfter": cf.publishedAfter,
            "publishedBefore": cf.publishedBefore,
            "type": "video",
            "maxResults": min(max_results, total_count - total_fetched),
            "order": cf.order,
            "regionCode": region_code
        }

        if next_page_token:
            params["pageToken"] = next_page_token

        try:
            response = requests.get(base_url, params=params)
            response.raise_for_status()
            data = response.json()

            # ì²« ë²ˆì§¸ ìš”ì²­ì—ì„œ pageInfo ì €ì¥
            if total_fetched == 0:
                page_info = data.get("pageInfo", {})
                total_results_from_api = page_info.get("totalResults", 0)
                results_per_page_from_api = page_info.get("resultsPerPage", 0)

            items = data.get("items", [])
            for item in items:
                # video ID ì¶”ì¶œ
                if "id" in item and "videoId" in item["id"]:
                    video_ids.append(item["id"]["videoId"])

                # ì›ë³¸ ë°ì´í„° ì €ì¥ (keyword_search.py í˜•ì‹)
                filtered_item = {
                    "etag": item.get("etag"),
                    "id": item.get("id"),
                    "snippet": {
                        "publishedAt": item["snippet"].get("publishedAt"),
                        "channelId": item["snippet"].get("channelId"),
                        "title": item["snippet"].get("title"),
                        "description": item["snippet"].get("description"),
                        "channelTitle": item["snippet"].get("channelTitle"),
                        "publishTime": item["snippet"].get("publishTime")
                    }
                }
                all_items.append(filtered_item)

            total_fetched += len(items)
            print(f"[ê²€ìƒ‰] ê°€ì ¸ì˜¨ ë°ì´í„°: {len(items)}ê°œ (ì´: {total_fetched}ê°œ)")

            next_page_token = data.get("nextPageToken")

            if not next_page_token or total_fetched >= total_count:
                break

        except requests.exceptions.RequestException as e:
            print(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

    page_info = {
        "totalResults": total_results_from_api,
        "resultsPerPage": results_per_page_from_api
    }

    return video_ids, all_items, search_params, page_info

def is_valid_content(title, description):
    """ì œëª©ê³¼ ì„¤ëª…ì— ê¸ˆì§€ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    # ê³µë°±ì„ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
    text = (str(title) + " " + str(description)).replace(" ", "").lower()
    for word in cf.STOPWORDS:
        if word in text:
            return False
    return True

def get_duration_seconds(duration_str):
    """ISO 8601 durationì„ ì´ˆë¡œ ë³€í™˜ (ì˜ˆ: PT2M30S â†’ 150ì´ˆ)""" # <----- duration ë³€í™˜ í•¨ìˆ˜ ì¶”ê°€
    import re
    if not duration_str:
        return 0
    match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
    if match:
        hours = int(match.group(1) or 0)
        minutes = int(match.group(2) or 0)
        seconds = int(match.group(3) or 0)
        return hours * 3600 + minutes * 60 + seconds
    return 0


'''
def get_video_details(video_ids):
    """
    YouTube ë™ì˜ìƒ ì„¸ë¶€ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ í•„ìš”í•œ í•„ë“œë§Œ ë°˜í™˜

    Args:
        video_ids (list): ë¹„ë””ì˜¤ ID ë¦¬ìŠ¤íŠ¸

    Returns:
        list: ì •ë¦¬ëœ ë¹„ë””ì˜¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """

    base_url = "https://www.googleapis.com/youtube/v3/videos"
    all_items = []
    batch_size = 50

    print("-" * 50)

    for i in range(0, len(video_ids), batch_size):
        batch_ids = video_ids[i:i+batch_size]

        params = {
            "key": API_KEY,
            "part": "snippet,statistics,contentDetails",
            "id": ",".join(batch_ids)
        }

        try:
            response = requests.get(base_url, params=params)
            response.raise_for_status()
            data = response.json()

            items = data.get("items", [])

            for item in items:
                statistics = item.get("statistics", {})
                cleaned_item = {
                    "id": item.get("id"),
                    "publishedAt": item.get("snippet", {}).get("publishedAt"),
                    "title": item.get("snippet", {}).get("title"),
                    "description": item.get("snippet", {}).get("description"),
                    "channelTitle": item.get("snippet", {}).get("channelTitle"),
                    "categoryId": item.get("snippet", {}).get("categoryId"),
                    "viewCount": statistics.get("viewCount"),
                    "likeCount": statistics.get("likeCount"),
                    "commentCount": statistics.get("commentCount")
                }
                all_items.append(cleaned_item)

            print(f"[ìƒì„¸] ê°€ì ¸ì˜¨ ë°ì´í„°: {len(items)}ê°œ (ì´: {len(all_items)}ê°œ)")

        except requests.exceptions.RequestException as e:
            print(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

    return all_items
'''

def get_video_details(video_ids):
    """
    YouTube ë™ì˜ìƒ ì„¸ë¶€ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ í•„ìš”í•œ í•„ë“œë§Œ ë°˜í™˜
    + ê°œì„ ëœ ë‹¤ë‹¨ê³„ í•„í„°ë§ ì ìš© # <----- ê°œì„ ëœ í•„í„°ë§
    """

    base_url = "https://www.googleapis.com/youtube/v3/videos"
    all_items = []
    batch_size = 50
    
    # í•„í„°ë§ í†µê³„ # <----- í•„í„°ë§ í†µê³„ ì¶”ì 
    stats = {
        'total': 0,
        'filtered_channel': 0,
        'filtered_stopwords': 0,
        'filtered_title': 0,
        'filtered_duration': 0,
        'filtered_ads': 0,
        'passed': 0
    }

    print("-" * 50)

    for i in range(0, len(video_ids), batch_size):
        batch_ids = video_ids[i:i+batch_size]

        params = {
            "key": API_KEY,
            "part": "snippet,statistics,contentDetails",  
            "id": ",".join(batch_ids)
        }

        try:
            response = requests.get(base_url, params=params)
            response.raise_for_status()
            data = response.json()
            items = data.get("items", [])
            
            stats['total'] += len(items)

            for item in items:
                snippet = item.get("snippet", {})
                title = snippet.get("title", "")
                description = snippet.get("description", "")
                channel_title = snippet.get("channelTitle", "")
                
                # 1ï¸âƒ£ ì±„ë„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„° # <----- ì±„ë„ í•„í„°
                if channel_title in cf.CHANNEL_BLACKLIST:
                    stats['filtered_channel'] += 1
                    continue
                
                # 2ï¸âƒ£ Stopwords í•„í„° (ê¸°ì¡´) # <----- ê¸ˆì§€ì–´ í•„í„°
                if not is_valid_content(title, description):
                    stats['filtered_stopwords'] += 1
                    continue
                
                # 3ï¸âƒ£ ì œëª©ì— í•„ìˆ˜ í‚¤ì›Œë“œ ì²´í¬ # <----- ì œëª© í‚¤ì›Œë“œ í•„í„°
                # if not any(kw in title for kw in cf.REQUIRED_IN_TITLE):
                #     stats['filtered_title'] += 1
                #     continue
                
                # 4ï¸âƒ£ ì˜ìƒ ê¸¸ì´ í™•ì¸ # <----- ê¸¸ì´ í•„í„°
                content_details = item.get("contentDetails", {})
                duration = content_details.get("duration", "")
                duration_seconds = get_duration_seconds(duration)
                
                if cf.FILTER_SHORTS and duration_seconds < cf.MIN_DURATION:
                    stats['filtered_duration'] += 1
                    continue
                
                # 5ï¸âƒ£ í˜‘ì°¬ í•„í„° # <----- í˜‘ì°¬ í•„í„°
                if cf.FILTER_ADS:
                    text = (title + " " + description).lower()
                    if any(kw.lower() in text for kw in cf.AD_KEYWORDS):
                        stats['filtered_ads'] += 1
                        continue

                # í†µê³¼! # <----- í†µê³¼í•œ ì˜ìƒë§Œ ì €ì¥
                stats['passed'] += 1
                statistics = item.get("statistics", {})

                cleaned_item = {
                    "id": item.get("id"),

                    # --- snippet ---
                    "publishedAt": snippet.get("publishedAt"),
                    "title": snippet.get("title"),
                    "description": snippet.get("description"),
                    "channelTitle": snippet.get("channelTitle"),
                    "categoryId": snippet.get("categoryId"),  
                    "tags": ",".join(snippet.get("tags", [])),  

                    # --- statistics ---
                    "viewCount": statistics.get("viewCount"),
                    "likeCount": statistics.get("likeCount"),
                    "commentCount": statistics.get("commentCount"),

                    # --- contentDetails ---
                    "duration": content_details.get("duration"),  
                    "licensedContent": content_details.get("licensedContent")  
                }

                all_items.append(cleaned_item)

            # ë°°ì¹˜ë³„ í†µê³„ ì¶œë ¥ # <----- ìƒì„¸ í†µê³„ ì¶œë ¥
            print(f"[ìƒì„¸] ë°°ì¹˜ {i//batch_size + 1}: {len(items)}ê°œ ì¤‘ {stats['passed']-len(all_items)+len(items)}ê°œ ìœ ì§€")

        except requests.exceptions.RequestException as e:
            print(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")
            break
    
    # ìµœì¢… í•„í„°ë§ í†µê³„ # <----- ìµœì¢… í†µê³„ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“Š í•„í„°ë§ í†µê³„")
    print("="*70)
    print(f"  â€¢ ì´ ì˜ìƒ: {stats['total']}ê°œ")
    print(f"  â€¢ ì±„ë„ ì œì™¸: {stats['filtered_channel']}ê°œ")
    print(f"  â€¢ ê¸ˆì§€ì–´ ì œì™¸: {stats['filtered_stopwords']}ê°œ")
    print(f"  â€¢ ì œëª© ë¶ˆì¼ì¹˜: {stats['filtered_title']}ê°œ")
    print(f"  â€¢ ê¸¸ì´ ë¶€ì¡±: {stats['filtered_duration']}ê°œ")
    print(f"  â€¢ í˜‘ì°¬ ì œì™¸: {stats['filtered_ads']}ê°œ")
    print(f"  âœ… ìµœì¢… í†µê³¼: {stats['passed']}ê°œ ({stats['passed']/stats['total']*100:.1f}%)")
    print("="*70 + "\n")

    return all_items


def save_search_result(search_items, search_params, page_info, folder_path, region_code="KR"):
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (keyword_search.py í˜•ì‹)

    Args:
        search_items (list): ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        search_params (dict): ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        page_info (dict): í˜ì´ì§€ ì •ë³´
        folder_path (str): ì €ì¥í•  í´ë” ê²½ë¡œ
        region_code (str): ì§€ì—­ ì½”ë“œ

    Returns:
        str: ì €ì¥ëœ íŒŒì¼ëª…
    """

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    query = search_params["query"].replace(" ", "").replace("|", "_")
    filename = os.path.join(folder_path, f"{query}_{timestamp}.json")

    result = {
        "searchParams": search_params,
        "kind": "youtube#searchListResponse",
        "regionCode": region_code,
        "pageInfo": page_info,
        "items": search_items
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"[ê²€ìƒ‰ ê²°ê³¼ ì €ì¥] {filename}")

    return filename


def save_cleaned_csv(cleaned_items, search_filename, folder_path):
    """
    ì •ë¦¬ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥

    Args:
        cleaned_items (list): ì •ë¦¬ëœ ë¹„ë””ì˜¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        search_filename (str): ê²€ìƒ‰ ê²°ê³¼ íŒŒì¼ëª… (CSV íŒŒì¼ëª… ìƒì„±ìš©)
        folder_path (str): ì €ì¥í•  í´ë” ê²½ë¡œ

    Returns:
        str: ì €ì¥ëœ íŒŒì¼ëª…
    """

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # íŒŒì¼ëª…ì—ì„œ í´ë” ê²½ë¡œ ì œê±° í›„ ì²˜ë¦¬
    base_name = os.path.basename(search_filename).replace(".json", "")
    filename = os.path.join(folder_path, f"{base_name}_data_{timestamp}.csv")

    # CSV ì»¬ëŸ¼ ìˆœì„œ
    fieldnames = [
        "id",
        "publishedAt",
        "title",
        "description",
        "channelTitle",
        "categoryId",
        "tags",              
        "duration",          
        "licensedContent",   
        "viewCount",
        "likeCount",
        "commentCount"
    ]
    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(cleaned_items)

    print(f"[ì •ë¦¬ ë°ì´í„° ì €ì¥] {filename}")

    return filename


# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    # from dateutil.relativedelta import relativedelta  # êµ¬ê°„ ë¶„í•  ì œê±°ë¡œ ë¶ˆí•„ìš”

    total_videos = 0
    
    # ì „ì²´ í‚¤ì›Œë“œ ìˆœíšŒ 
    for keyword_idx, keyword in enumerate(cf.KEYWORDS, 1): 
        print(f"\n{'#'*70}")
        print(f"í‚¤ì›Œë“œ [{keyword_idx}/{len(cf.KEYWORDS)}]: {keyword}") 
        print(f"{'#'*70}")

        # data/í‚¤ì›Œë“œëª… í´ë” ìƒì„± (ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # í´ë”ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° 
        safe_keyword = keyword.split()[0] if ' ' in keyword else keyword # ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©
        folder_name = os.path.join(script_dir, "data", safe_keyword.replace(" ", "").replace("|", ""))
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)
            print(f"í´ë” ìƒì„±: {folder_name}/")

        # êµ¬ê°„ ë¶„í•  ì œê±°: ì „ì²´ ê¸°ê°„ì„ í•œ ë²ˆì— ê²€ìƒ‰ 
        print(f"\n{'='*50}")
        print(f"ì „ì²´ ê¸°ê°„ ê²€ìƒ‰: {cf.publishedAfter} ~ {cf.publishedBefore}") 
        print(f"{'='*50}")

        # 1. YouTube ê²€ìƒ‰ (êµ¬ê°„ ë¶„í•  ì—†ì´ í•œ ë²ˆì— ì‹¤í–‰) <-----
        video_ids, search_items, search_params, page_info = search_youtube(
            query=keyword,
            publishedAfter=cf.publishedAfter,  # <----- ì „ì²´ ê¸°ê°„ ì‹œì‘
            publishedBefore=cf.publishedBefore,  # <----- ì „ì²´ ê¸°ê°„ ì¢…ë£Œ
            total_count=cf.total_count,
            max_results=50,
            order=cf.order,
            region_code="KR"
        )

        if not video_ids:
            print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.") # <-----
            continue  # <----- ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ë„˜ì–´ê°

        # 2. ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (JSON)
        search_filename = save_search_result(search_items, search_params, page_info, folder_name)

        # 3. ë¹„ë””ì˜¤ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cleaned_items = get_video_details(video_ids)

        # 4. ì •ë¦¬ëœ ë°ì´í„° ì €ì¥ (CSV)
        cleaned_filename = save_cleaned_csv(cleaned_items, search_filename, folder_name)

        total_videos += len(cleaned_items)
        print(f"{len(cleaned_items)}ê°œì˜ ë¹„ë””ì˜¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        
        # í‚¤ì›Œë“œë³„ êµ¬ë¶„ì„  
        print(f"\n{'#'*70}") 
        print(f"âœ… í‚¤ì›Œë“œ [{keyword_idx}/{len(cf.KEYWORDS)}] ì™„ë£Œ: {keyword}") 
        print(f"{'#'*70}\n") 

    print(f"\n{'='*70}") 
    print(f"ğŸ‰ ì „ì²´ ì™„ë£Œ: ì´ {total_videos}ê°œì˜ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.") 
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {script_dir}/data/") 
    print(f"ğŸ”‘ ì²˜ë¦¬í•œ í‚¤ì›Œë“œ ìˆ˜: {len(cf.KEYWORDS)}ê°œ") 
    print(f"{'='*70}")